---
📅 **날짜**: 2025년 1월 21일 (화)
👤 **작성자**: 강철원 (연구책임자) | **승인**: 강혜림 (대표)
📊 **진행 단계**: 2단계 - 핵심기술개발
🎯 **주요 작업**: GPU 메모리 최적화
---

# AI 기반 다국어 음성 합성 및 실시간 립싱크 더빙 시스템 개발일지

## 📋 오늘의 작업 내용

### 1. 모델 언로딩 (Unloading)

- VRAM 12GB 환경에서 Whisper(Large), VALL-E X, RVC를 모두 올리면 OOM 발생 위험.
- 각 단계가 끝나면 `del model`, `torch.cuda.empty_cache()`를 호출하여 메모리 확보.
- **단점**: 다음 요청 시 모델을 다시 로드해야 하므로 레이턴시 증가.
- **타협**: Whisper는 상주시키고, TTS/RVC는 필요 시 로드/언로드.

### 2. 구현

- `ResourceManager` 클래스 구현하여 모델 수명 주기 관리.

## 🔧 기술적 진행사항

### Memory Management

```python
import gc
def unload_model(model):
    del model
    gc.collect()
    torch.cuda.empty_cache()
```

## 📊 진행 상황

| 항목 | 계획 | 실제 | 상태 |
|------|------|------|------|
| 언로딩 구현 | 완료 | 완료 | ✅ |
| 테스트 | 완료 | 완료 | ✅ |

## 🚧 이슈 사항 및 해결 방안

- **특이사항 없음**.

## 📝 내일 계획

1. 배치 처리 도입
2. 설 연휴 대비 점검

---

## 📚 참고 자료

- [1] "PyTorch Memory Management".

<details>
<summary>IRIS 붙여넣기용 HTML 코드</summary>

```html
<h3>1. 모델 언로딩 (Unloading)</h3>
<ul>
<li>VRAM 12GB 환경에서 Whisper(Large), VALL-E X, RVC를 모두 올리면 OOM 발생 위험.</li>
<li>각 단계가 끝나면 <code>del model</code>, <code>torch.cuda.empty_cache()</code>를 호출하여 메모리 확보.</li>
<li><strong>단점</strong>: 다음 요청 시 모델을 다시 로드해야 하므로 레이턴시 증가.</li>
<li><strong>타협</strong>: Whisper는 상주시키고, TTS/RVC는 필요 시 로드/언로드.</li>
</ul>
<h3>2. 구현</h3>
<ul>
<li><code>ResourceManager</code> 클래스 구현하여 모델 수명 주기 관리.</li>
</ul>
```

</details>
