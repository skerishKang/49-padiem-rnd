---
π“… **λ‚ μ§**: 2025λ…„ 6μ›” 5μΌ (λ©)
π‘¤ **μ‘μ„±μ**: κ°•μ² μ› (μ—°κµ¬μ±…μ„μ) | **μΉμΈ**: κ°•νλ¦Ό (λ€ν‘)
π“ **μ§„ν–‰ λ‹¨κ³„**: 3λ‹¨κ³„ - μ‹μ¤ν… ν†µν•© λ° κ³ λ„ν™”
π― **μ£Όμ” μ‘μ—…**: STT λ¨λ“ μ—°λ™ λ° UI κµ¬ν„
---

# AI κΈ°λ° λ‹¤κµ­μ–΄ μμ„± ν•©μ„± λ° μ‹¤μ‹κ°„ λ¦½μ‹±ν¬ λ”λΉ™ μ‹μ¤ν… κ°λ°μΌμ§€

## π“‹ μ¤λμ μ‘μ—… λ‚΄μ©

### 1. STT API κµ¬ν„

- `backend/routers/stt.py`: OpenAI Whisper λ¨λΈ λ΅λ“ λ° μ¶”λ΅ .
- **μµμ…**: λ¨λΈ ν¬κΈ°(base, small, medium) μ„ νƒ κ°€λ¥ν•λ„λ΅ νλΌλ―Έν„°ν™”.
- **κ²°κ³Ό**: ν…μ¤νΈ λ° νƒ€μ„μ¤νƒ¬ν”„(SRT ν•μ‹) λ°ν™.

### 2. Streamlit UI (1λ‹¨κ³„)

- `frontend/steps/audio.py`: νμΌ μ—…λ΅λ” μ„μ ― κµ¬ν„.
- `frontend/steps/stt.py`: STT μ‹¤ν–‰ λ²„νΌ λ° κ²°κ³Ό ν…μ¤νΈ μ—λ””ν„° κµ¬ν„.

## π”§ κΈ°μ μ  μ§„ν–‰μ‚¬ν•­

### Whisper Integration

```python
model = whisper.load_model("base")
result = model.transcribe(audio_path)
```

## π“ μ§„ν–‰ μƒν™©

| ν•­λ© | κ³„ν | μ‹¤μ  | μƒνƒ |
|------|------|------|------|
| STT API | μ™„λ£ | μ™„λ£ | β… |
| UI κµ¬ν„ | μ™„λ£ | μ™„λ£ | β… |

## π§ μ΄μ μ‚¬ν•­ λ° ν•΄κ²° λ°©μ•

- **GPU λ©”λ¨λ¦¬**: Whisper λ¨λΈ λ΅λ“ μ‹ VRAM μ μ μ¨ μƒμΉ. -> μ‚¬μ© ν›„ `del model`, `torch.cuda.empty_cache()` νΈμ¶ν•μ—¬ λ©”λ¨λ¦¬ ν•΄μ .

## π“ λ‚΄μΌ κ³„ν

1. λ²μ—­ λ¨λ“ (Deep Translator) μ—°λ™
2. TTS (VALL-E X) μ—°λ™ μ¤€λΉ„

---

## π“ μ°Έκ³  μλ£

- [1] "OpenAI Whisper". [Link](https://github.com/openai/whisper)

<details>
<summary>IRIS λ¶™μ—¬λ„£κΈ°μ© HTML μ½”λ“</summary>

```html
<h3>1. STT API κµ¬ν„</h3>
<ul>
<li><code>backend/routers/stt.py</code>: OpenAI Whisper λ¨λΈ λ΅λ“ λ° μ¶”λ΅ .</li>
<li><strong>μµμ…</strong>: λ¨λΈ ν¬κΈ°(base, small, medium) μ„ νƒ κ°€λ¥ν•λ„λ΅ νλΌλ―Έν„°ν™”.</li>
<li><strong>κ²°κ³Ό</strong>: ν…μ¤νΈ λ° νƒ€μ„μ¤νƒ¬ν”„(SRT ν•μ‹) λ°ν™.</li>
</ul>
<h3>2. Streamlit UI (1λ‹¨κ³„)</h3>
<ul>
<li><code>frontend/steps/audio.py</code>: νμΌ μ—…λ΅λ” μ„μ ― κµ¬ν„.</li>
<li><code>frontend/steps/stt.py</code>: STT μ‹¤ν–‰ λ²„νΌ λ° κ²°κ³Ό ν…μ¤νΈ μ—λ””ν„° κµ¬ν„.</li>
</ul>
```

</details>
