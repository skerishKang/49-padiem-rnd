---
📅 **날짜**: 2025년 7월 11일 (금)
👤 **작성자**: 강철원 (연구책임자) | **승인**: 강혜림 (대표)
📊 **진행 단계**: 3단계 - 시스템 통합 및 고도화
🎯 **주요 작업**: 성능 최적화 (캐싱)
---

# AI 기반 다국어 음성 합성 및 실시간 립싱크 더빙 시스템 개발일지

## 📋 오늘의 작업 내용

### 1. 모델 캐싱

- AI 모델(Whisper, VALL-E X, RVC) 로드 시간이 오래 걸림 (약 10~20초).
- **해결**: Streamlit의 `@st.cache_resource` 데코레이터를 사용하여 모델을 메모리에 상주시키고 재사용.
- **효과**: 두 번째 실행부터는 모델 로드 시간 0초.

### 2. 데이터 캐싱

- 동일한 오디오에 대한 STT 결과 등을 캐싱.
- `@st.cache_data` 활용.

## 🔧 기술적 진행사항

### Streamlit Caching

```python
@st.cache_resource
def load_whisper_model():
    return whisper.load_model("base")
```

## 📊 진행 상황

| 항목 | 계획 | 실제 | 상태 |
|------|------|------|------|
| 모델 캐싱 | 완료 | 완료 | ✅ |
| 데이터 캐싱 | 완료 | 완료 | ✅ |

## 🚧 이슈 사항 및 해결 방안

- **GPU 메모리**: 모델을 계속 상주시킬 경우 VRAM 부족 우려. -> LRU(Least Recently Used) 정책 적용 또는 명시적 해제 버튼 추가.

## 📝 다음 주 계획 (07.14 ~ 07.18)

1. 통합 테스트 (최적화 적용 버전)
2. 사용자 매뉴얼 작성

---

## 📚 참고 자료

- [1] "Streamlit Caching". [Link](https://docs.streamlit.io/library/advanced-features/caching)

<details>
<summary>IRIS 붙여넣기용 HTML 코드</summary>

```html
<h3>1. 모델 캐싱</h3>
<ul>
<li>AI 모델(Whisper, VALL-E X, RVC) 로드 시간이 오래 걸림 (약 10~20초).</li>
<li><strong>해결</strong>: Streamlit의 <code>@st.cache_resource</code> 데코레이터를 사용하여 모델을 메모리에 상주시키고 재사용.</li>
<li><strong>효과</strong>: 두 번째 실행부터는 모델 로드 시간 0초.</li>
</ul>
<h3>2. 데이터 캐싱</h3>
<ul>
<li>동일한 오디오에 대한 STT 결과 등을 캐싱.</li>
<li><code>@st.cache_data</code> 활용.</li>
</ul>
```

</details>
