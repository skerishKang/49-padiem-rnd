---
π“… **λ‚ μ§**: 2025λ…„ 7μ›” 14μΌ (μ›”)
π‘¤ **μ‘μ„±μ**: κ°•μ² μ› (μ—°κµ¬μ±…μ„μ) | **μΉμΈ**: κ°•νλ¦Ό (λ€ν‘)
π“ **μ§„ν–‰ λ‹¨κ³„**: 3λ‹¨κ³„ - κ³ λ„ν™” λ° μ‚¬μ—…ν™”
π― **μ£Όμ” μ‘μ—…**: Llama 3 λ¨λΈ μ„ μ •
---

# AI κΈ°λ° λ‹¤κµ­μ–΄ μμ„± ν•©μ„± λ° μ‹¤μ‹κ°„ λ¦½μ‹±ν¬ λ”λΉ™ μ‹μ¤ν… κ°λ°μΌμ§€

## π“‹ μ¤λμ μ‘μ—… λ‚΄μ©

### 1. λ¨λΈ μ„ μ •

- **ν›„λ³΄**: Meta Llama 3 8B, 70B.
- **μ”κµ¬μ‚¬ν•­**: ν•κµ­μ–΄ μ²λ¦¬ λ¥λ ¥μ΄ μ°μν•΄μ•Ό ν•¨.
- **κ²°μ •**: **Llama-3-Open-Ko-8B** (ν•κµ­μ–΄ νμΈνλ‹ λ²„μ „). 70Bλ” μ¶”λ΅  λΉ„μ©μ΄ λ„λ¬΄ λ†’μ•„ 8Bλ΅ μ‹μ‘.

### 2. ν™κ²½ κµ¬μ¶•

- **GPU**: NVIDIA A10G (24GB VRAM) μΈμ¤ν„΄μ¤ λ€μ—¬.
- **λΌμ΄λΈλ¬λ¦¬**: `vllm` (κ³ μ† μ¶”λ΅  μ—”μ§„).

## π”§ κΈ°μ μ  μ§„ν–‰μ‚¬ν•­

### vLLM μ„¤μΉ λ° μ‹¤ν–‰

```bash
pip install vllm
python -m vllm.entrypoints.openai.api_server --model "beomi/Llama-3-Open-Ko-8B" --port 8000
```

## π“ μ§„ν–‰ μƒν™©

| ν•­λ© | κ³„ν | μ‹¤μ  | μƒνƒ |
|------|------|------|------|
| λ¨λΈ μ„ μ • | μ™„λ£ | μ™„λ£ | β… |
| μ„λ²„ μ‹¤ν–‰ | μ™„λ£ | μ™„λ£ | β… |

## π§ μ΄μ μ‚¬ν•­ λ° ν•΄κ²° λ°©μ•

- **ν† ν¬λ‚μ΄μ €**: ν•κµ­μ–΄ ν† ν° μ²λ¦¬κ°€ μ›ν™ν•μ§€ ν™•μΈ ν•„μ”. -> μƒν” ν…μ¤νΈλ΅ ν…μ¤νΈ κ²°κ³Ό μ–‘νΈ.

## π“ λ‚΄μΌ κ³„ν

1. λ²μ—­ ν”„λ΅¬ν”„νΈ μ—”μ§€λ‹μ–΄λ§
2. DeepLκ³Ό ν’μ§ λΉ„κµ

---

## π“ μ°Έκ³  μλ£

- [1] "Llama 3 Open Ko". [Link](https://huggingface.co/beomi/Llama-3-Open-Ko-8B)

<details>
<summary>IRIS λ¶™μ—¬λ„£κΈ°μ© HTML μ½”λ“</summary>

```html
<h3>1. λ¨λΈ μ„ μ •</h3>
<ul>
<li><strong>ν›„λ³΄</strong>: Meta Llama 3 8B, 70B.</li>
<li><strong>μ”κµ¬μ‚¬ν•­</strong>: ν•κµ­μ–΄ μ²λ¦¬ λ¥λ ¥μ΄ μ°μν•΄μ•Ό ν•¨.</li>
<li><strong>κ²°μ •</strong>: <strong>Llama-3-Open-Ko-8B</strong> (ν•κµ­μ–΄ νμΈνλ‹ λ²„μ „). 70Bλ” μ¶”λ΅  λΉ„μ©μ΄ λ„λ¬΄ λ†’μ•„ 8Bλ΅ μ‹μ‘.</li>
</ul>
<h3>2. ν™κ²½ κµ¬μ¶•</h3>
<ul>
<li><strong>GPU</strong>: NVIDIA A10G (24GB VRAM) μΈμ¤ν„΄μ¤ λ€μ—¬.</li>
<li><strong>λΌμ΄λΈλ¬λ¦¬</strong>: <code>vllm</code> (κ³ μ† μ¶”λ΅  μ—”μ§„).</li>
</ul>
```

</details>
