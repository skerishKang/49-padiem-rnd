# 연구노트 (책임자 관점)

- **날짜**: 2024-10-21
- **작성자**: 강혜림
- **역할**: 책임연구원
- **진행 단계**: 1단계 기초연구·설계
- **아젠다**: chunked inference 설계·구현, long clip 안정성 확보

---

## 1. 연구 배경·목적 정리 (책임 관점)

- 10/18 orchestrator 통합 테스트에서 long clip 처리 실패가 발생한 문제를 해결하기 위해, **chunked inference 로직**을 설계·구현하는 날.
- 목표는
  - 30초 이상 길이의 clip에서도 안정적으로 추론이 가능하도록 하고,
  - 처리 시간 증가는 최소화하면서 메모리 피크와 실패율을 0에 가깝게 줄이는 것.

---

## 2. 오늘 진행 내용 (세부)

### 2.1 chunked inference 설계
- 입력 오디오/영상 클립을 10초 단위로 분할해 VALL-E X와 Wav2Lip에 순차 공급.
- 인접 chunk 간의 문맥 유지를 위해 context window를 1초씩 중첩(overlap)하도록 설계.
- 출력 단계에서 overlap 구간을 다시 병합(overlap-add)하는 방식으로 자연스러운 연결을 확보.

### 2.2 상태관리 및 재시도 로직
- orchestrator에서 각 chunk의 상태를 추적하는 테이블(메모리/로그)을 추가.
- 특정 chunk 실패 시 재시도·스킵·경고 발생 등 fallback 전략을 정의.

### 2.3 테스트 및 결과
- 45초/60초/90초 clip 6건을 대상으로 테스트.
- 이전에 실패했던 케이스도 모두 성공적으로 처리.
- 평균 처리 시간은 약 7분 20초 → 8분 수준으로 약간 증가했으나, long clip 실패율은 0%로 개선.
- GPU 메모리 피크는 41GB 수준으로 관리 가능한 범위에 머무름.

### 2.4 릴리스 노트 정리
- `docs/releases/20241021_chunked_inference.md`를 작성해 설계 개요, 파라미터( chunk 길이, overlap, 재시도 정책 등), 주의사항을 문서화.

---

## 3. 책임자 관점 의사결정·리스크

- **결정 1: chunked inference 정식 채택**
  - long clip 안정성을 IRIS 과제 품질 포인트 중 하나로 삼기 위해, chunked inference를 기본 경로로 채택.

- **리스크 1: 처리 시간 증가**
  - 평균 처리시간이 약간 증가(7분 20초 → 8분).
  - → **대응**: 추후 hybrid 모드(일반 clip은 single-pass, 긴 clip만 chunk 모드)를 도입해, 전체 평균을 낮추는 방향으로 10/23에 실험 예정.

---

## 4. 지표·산출물 정리

- **성능 지표**
  - long clip 성공률: 100%(6/6).
  - 평균 처리 시간: 약 8분.
  - GPU 메모리 피크: 41GB.
- **문서**
  - `20241021_chunked_inference.md` 릴리스 노트.

---

## 5. 다음 액션 (책임 관점 To-do)

1. **10/23 hybrid 전략 튜닝**
   - 30초 기준으로 single vs chunk 모드를 분기하는 하이브리드 전략 실험.

2. **10/24 IRIS 중간점검 리허설 준비**
   - chunked inference 결과를 기반으로, long clip 시나리오를 IRIS 중간점검 데모에 포함.

---

## 6. 참고 문서·링크

- IRIS 연구계획서 PART2(대용량 데이터 처리).
- NVIDIA Streaming Inference Best Practices.

> 메모: 이 노트는 2024-10-21 원본 연구노트의 chunked inference 설계 내용을 책임자 관점에서 정리한 것이다. 이후 10/23 하이브리드 전략 튜닝과 IRIS 중간점검 리허설의 기술적 근거가 된다.