# 최종보고서(본문)

- 연구개발과제명: AI 기반 다국어 음성 합성 및 실시간 립싱크 더빙 시스템 개발
- 연구개발과제번호: RS-2024-00511307
- 연구개발기간: 2024.10.01 ~ 2025.09.30

## 목차

- 1. 연구개발과제의 개요
- 2. 연구개발과제의 수행 과정 및 수행내용
- 3. 연구개발과제의 수행 결과 및 목표 달성 정도
- 4. 목표 미달 시 원인분석(해당 시 작성)
- 5. 연구개발성과 및 관련 분야에 대한 기여 정도
- 6. 연구개발성과의 관리 및 활용 계획
- 별첨자료(참고 문헌 등)

# 1. 연구개발과제의 개요

## 1-1. 개발 배경 및 필요성

본 연구개발과제는 중소벤처기업부 창업성장기술개발사업(디딤돌)의 일환으로 추진된 과제로서, AI 기반 다국어 음성 합성 및 실시간 립싱크 더빙 시스템을 이용하여 영상 콘텐츠의 다국어 더빙을 One-Stop으로 자동 처리할 수 있는 플랫폼을 구축하고자 하였음.

최근 OTT 및 유튜브 등 글로벌 콘텐츠 시장이 급성장함에 따라 다국어 더빙 수요가 증가하고 있으나, 기존 더빙 제작 방식은 언어별 스크립트 작성, 전문 성우 섭외 및 녹음, 영상 편집 및 입 모양(Lip-sync) 싱크 작업을 단계별 수작업으로 처리해야 하는 구조적 한계가 있음. 이로 인해 제작 비용이 높고 제작 기간이 길어, 자금력과 인력이 제한된 중소기업 및 1인 창작자가 글로벌 시장에 진출하는 데 큰 진입 장벽으로 작용하였음.

이에 본 과제에서는 최신 AI 기술을 융합하여 번역부터 음성 합성, 립싱크까지 전 과정을 자동화함으로써, 기존 대비 비용과 시간을 절감하고 누구나 쉽게 고품질 다국어 콘텐츠를 제작할 수 있는 솔루션을 개발하고자 하였음.

## 1-2. 최종 목표

과제의 세부 목표는 연구개발계획서에서 제시한 바와 같이 다음 4가지를 핵심 지표로 설정하였음.

- 고품질 AI 음성 합성 기술 개발
  - 원화자의 음색과 감정(억양/발화 습관)을 보존하면서도 자연스러운 다국어 음성을 생성
  - 목표(계획서): 음성 유사도 10% 향상, 학습 시간 20% 단축
- AI 기반 립싱크 기술 고도화
  - 합성된 음성에 맞춰 영상 속 인물의 입 모양을 정교하게 동기화
  - 목표(계획서): 립싱크 정확도 10% 개선, 처리 속도 20% 단축
- AI 더빙 워크플로우 최적화
  - 음성 추출부터 렌더링까지의 전체 공정을 파이프라인화하여 작업 효율을 극대화
  - 목표(계획서): 전체 과정 30% 시간 단축, 주요 2단계 자동화
- AI 더빙 품질 관리 시스템 개발
  - 결과물 품질을 정량적으로 측정(WER/CER, LSE-D/LSE-C 등)하고 피드백할 수 있는 체계를 구축
  - 목표(계획서): 자동 평가 시스템 구축, 평가 정확도 85% 달성
- 근거(내부): `pdf/ocr/본문1 (2)/과제접수용연구개발계획서(PART2(본문1))_page-0001.jpg`, `pdf/ocr/본문1 (2)/과제접수용연구개발계획서(PART2(본문1))_page-0003.jpg`

## 1-3. 주요 연구 내용 및 차별성

본 과제는 단순히 여러 모델을 조합하는 수준을 넘어, 영상 입력부터 최종 더빙 영상 산출까지의 흐름을 재현 가능한 모듈로 구조화하고 운영 가능한 형태로 통합한 점에 차별성이 있음.

### 1-3-1. End-to-End One-Stop 자동화 프로세스 설계

- 영상 입력부터 최종 편집(렌더링/QC 포함)까지 전체 공정을 세분화하여 자동화 가능한 형태로 설계하였음.
- 모듈 간 데이터 표준(JSON 기반 타임스탬프/자막 포맷 등)을 정의하여 단계별 산출물이 다음 단계에 자연스럽게 이어지도록 구성하였음.

### 1-3-2. 하이브리드 AI 모델 활용(단계별 최적 모델 모듈화)

- STT: Whisper 기반 로컬 STT를 기본으로 하고, 난이도 높은 발화/전문 용어의 경우 Gemini API를 선택적으로 활용하는 이중화 체계를 구성하였음.
- TTS: VALL-E X, XTTS 등 다국어 TTS 모델을 적용하고, 서비스 환경에 맞는 튜닝/운용 전략을 수립하였음.
- Lip-Sync: Wav2Lip 기반 립싱크 파이프라인에 더하여 MuseTalk을 추가 도입하여 고해상도 립싱크 품질을 확보하였음.

### 1-3-3. 화자 음색 보존(Voice Cloning)

RVC(Retrieval-based Voice Conversion) 기반 Voice Cloning 기술을 도입하여, 단순히 언어만 바꾸는 것이 아니라 원작자의 목소리 톤과 감정을 유지한 채 다른 언어로 말하는 듯한 고품질 더빙을 구현하고자 하였음.

### 1-3-4. 고성능/고화질 립싱크 파이프라인

기존 Wav2Lip 계열 모델의 한계를 보완하기 위해 MuseTalk을 추가 도입하였으며, GPU 활용 및 fp16 옵션(예: MuseTalk 설정의 `fp16: True`)을 통해 고화질 영상에서도 처리 효율을 확보하고자 하였음.

## 1-4. 추진 경과 및 기대 효과

본 과제는 2024.10.01.~2025.09.30. 기간 동안 주관기관((주)파디엠)과 공동연구기관(전북대학교 산학협력단)의 협력을 통해 수행되었음. 연구개발계획서 상 기술성숙도(TRL) 9단계 달성을 목표로 파이프라인과 핵심 모델을 단계적으로 고도화하였음.

기대 효과는 연구개발계획서에서 제시한 기대효과 및 장기 파급효과를 기준으로, 본 과제에서 구축한 End-to-End 파이프라인 및 핵심 모델 고도화 성과와의 연계를 중심으로 다음과 같이 정리할 수 있음.

- 기술적 효과
  - STT/번역/TTS/음색 변환/립싱크 등 분절된 공정을 단일 파이프라인으로 통합하여, 영상 입력부터 결과물 산출까지 One-Stop으로 수행 가능한 자동 더빙 제작 체계를 확보하였음.
  - Voice Cloning(RVC) 기반 음색 보존 및 다국어 TTS(VALL-E X/XTTS/Gemini) 하이브리드 운용을 통해, 단순 번역을 넘어 원화자의 음색·억양 특성을 유지하는 고품질 더빙 기반을 구축하였음.
  - MuseTalk/Wav2Lip 기반 립싱크를 통합하고 고해상도 처리 및 fp16 등 실행 파라미터를 튜닝하여, 기존 저해상도 한계를 보완하고 시각적 자연스러움(입 주변부 품질)을 개선하였음.
  - 단계별 산출물(타임스탬프 JSON, 자막 포맷 등)을 표준화하여 재현 가능하고 운영 가능한 형태로 워크플로우를 구조화하였음.
  - 계획서에서는 성능 목표(기대 수준)로서 음성 유사도 95% 이상(업계 표준 평가 기준 적용), 립싱크 정확도 98% 이상, 실시간 처리 속도 20% 개선 등을 제시하였음. 본 과제는 파이프라인을 모듈화하고 산출물 포맷을 표준화하여, 해당 정량 목표를 단계별로 계측·개선할 수 있는 구조(지표 기반 QC 운영)를 우선 확보하였음.
  - 또한 한국어·영어·중국어·일본어 등 주요 언어 간 자연스러운 변환을 목표로 하는 다국어 확장 방향을 제시함으로써, 콘텐츠 현지화·교육·접근성 영역으로의 적용 가능성을 확보하였음.

- 경제적 효과
  - 기존 수작업 더빙 공정(스크립트 작성, 녹음, 편집, 립싱크)을 자동화함으로써 제작 비용과 제작 기간을 절감하고, 특히 인력·자금 제약이 있는 중소 제작사 및 1인 창작자의 다국어 콘텐츠 제작 진입 장벽을 낮추는 효과를 기대할 수 있음.
  - 다국어 더빙 수요가 높은 글로벌 OTT/유튜브 등 플랫폼 환경에서, 다국어 버전 제작의 반복 비용을 낮추고 제작 리드타임을 단축하여 해외 시장 진출 및 사업 확장에 기여할 수 있음.
  - 계획서에서는 AI 더빙 기술 도입을 통해 콘텐츠 제작 비용을 30% 이상 절감하고, 다국어 버전 제작 기간을 50% 단축하는 등의 효과를 기대하였음. 본 과제의 자동화 파이프라인은 이러한 비용/기간 절감 효과를 측정·검증할 수 있는 운영 단위를 제공하였음.
  - 또한 사업화 KPI 예시로서 서비스 출시 후 첫 해 MAU 1만 명, 연간 1,000명 유료 구독자, 월 평균 5,000건 유료 더빙 작업 처리, B2B 10개 이상 기업 고객 확보 등을 제시하였음. 이는 본 과제에서 구축한 API/파이프라인 기반 자동화 체계를 바탕으로 단계적 상용화 과정에서 검증·확대 가능한 목표 지표임.

- 사회·문화적 효과
  - 다국어 교육 콘텐츠를 보다 손쉽게 제작·배포할 수 있게 함으로써 글로벌 교육 기회를 확대하고, 언어 장벽으로 인해 발생하는 교육 접근성 격차를 완화하는 데 기여할 수 있음.
  - 소규모 제작사의 콘텐츠도 다국어 버전 제작이 가능해져 문화 다양성 증진 및 문화 교류 활성화에 기여할 수 있으며, K-콘텐츠의 글로벌 확산을 가속화하여 한국 문화의 세계적 영향력 확대에 기여할 수 있음.
  - 교육/접근성(장애인·고령자 지원) 등 정보 접근성 향상에 기여하고, 다양한 사용자 집단이 영상·음성 기반 콘텐츠를 이해·활용하는 데 필요한 보조 수단을 제공할 수 있음.
  - 계획서에서는 개인화 AI 음성 튜터 등으로 언어 학습 효율성 20% 이상 향상 등 교육 분야 적용 가능성을 제시하였음. 이는 본 과제의 다국어 음성 합성/변환 기반을 활용하여 교육 콘텐츠 제작/배포를 고도화할 수 있음을 의미함.

- 윤리적·제도적 효과
  - AI 음성 합성 기술의 윤리적 사용 가이드라인을 제시하고, 기술 적용 과정에서의 사회적 신뢰를 확보할 수 있는 운영 원칙을 정립하는 효과를 기대할 수 있음.
  - 개인정보 보호 및 저작권 존중을 위한 기술적·운영적 대응(데이터 관리 체계, 사용 동의 기반 운영 등)을 병행함으로써 책임 있는 AI 활용 확산에 기여할 수 있음.
  - 또한 사칭/악용(예: 보이스피싱 등) 리스크 관점에서 운영 점검 프로세스를 병행하고, 사용자 동의 기반의 데이터 수집/활용 원칙을 정립함으로써 책임 있는 AI 적용을 지향하였음.

- 장기적 파급 효과
  - 글로벌 AI 음성 시장 선도: 연구개발계획서에서는 2026년까지 글로벌 AI 음성 시장의 10% 이상 점유율 달성(목표)을 제시하였으며, 이를 위해 국내 AI 기술의 해외 수출 및 국가 경쟁력 강화를 장기 파급효과로 설정하였음.
  - 메타버스/가상현실 발전 기여: 실시간 다국어 음성 변환 기술을 기반으로 글로벌 가상 공간에서의 원활한 소통을 지원하고, 개인화된 아바타 음성 생성 기술을 통해 몰입도 높은 사용자 경험을 제공할 수 있음.
  - AI 기술의 대중화 및 인식 개선: 일상에서 쉽게 접할 수 있는 AI 기반 더빙/번역 기술을 제공함으로써 기술 수용성을 높이고, AI와 인간의 협업 모델 확산을 통해 AI 기술에 대한 긍정적 인식을 확산할 수 있음.

- 근거(내부): `pdf/ocr/본문1 (2)/과제접수용연구개발계획서(PART2(본문1))_page-0023.jpg`(기대효과: 기술/경제/사회), `pdf/ocr/본문1 (2)/과제접수용연구개발계획서(PART2(본문1))_page-0021.jpg`, `pdf/ocr/본문1 (2)/과제접수용연구개발계획서(PART2(본문1))_page-0022.jpg`(사업화 전략 KPI 예시), `pdf/ocr/본문1 (2)/과제접수용연구개발계획서(PART2(본문1))_page-0024.jpg`(장기 파급효과)

# 2. 연구개발과제의 수행 과정 및 수행내용

## 2-1. 전체 파이프라인 설계 및 워크플로우 최적화

본 연구개발과제는 연구개발계획서에서 제시한 연차·단계별 추진계획에 따라, (1) 고품질 AI 음성 합성, (2) AI 기반 립싱크 고도화, (3) 워크플로우 최적화, (4) 품질 관리 시스템 개발의 4대 목표를 중심으로 수행되었음.

### 2-1-0. 파이프라인 모듈 구성 및 근거(코드 경로)

본 과제의 End-to-End 파이프라인은 단계별로 독립 실행 가능한 모듈(`modules/*/run.py`)로 구현하고, (1) Orchestrator에서 순차 실행하거나 (2) FastAPI 백엔드 API에서 모듈을 호출하도록 구성하였음.

다음 표는 본 보고서에서 기술하는 파이프라인 단계와 실제 코드/설정 파일의 대응 관계를 요약한 것으로, 본문 서술의 근거로 활용하였음.

| 구분 | 목적 | 구현/연동(근거) | 비고 |
| --- | --- | --- | --- |
| 오디오 추출 | 영상/오디오 입력에서 표준 WAV 추출 | `modules/audio_extractor/run.py`, `modules/audio_extractor/config/settings.yaml` | FFmpeg 경로/코덱/샘플레이트 설정 기반 |
| STT(Whisper) | 음성→텍스트 전사 | `modules/stt_whisper/run.py`, `modules/stt_whisper/config/settings*.yaml` | `openai/whisper` 기반 |
| STT(Gemini, 선택) | 난이도 높은 발화 보완용 STT | `modules/stt_gemini/run.py` | API 기반(선택 경로) |
| 텍스트 처리/번역 | 정규화·번역·길이/타이밍 정합 | `modules/text_processor/run.py`, `modules/text_processor/config/settings.yaml`, `docs/timed_translation_spec.md` | JSON 세그먼트 표준을 유지 |
| TTS(VALL-E X) | 텍스트→음성 합성(주) | `modules/tts_vallex/run.py`, `modules/tts_vallex/config/settings.yaml` | 외부 스크립트 실행 형태 |
| TTS(XTTS, 백업) | 텍스트→음성 합성(보조) | `modules/tts_xtts/run.py`, `modules/tts_xtts/config/settings.yaml` | 백업 경로 |
| TTS(Gemini, 선택) | API 기반 음성 합성 | `modules/tts_gemini/run.py` | 선택 경로 |
| 음색 변환(RVC) | 원화자 음색 유지(VC) | `modules/voice_conversion_rvc/run.py`, `modules/voice_conversion_rvc/config/settings.yaml` | 체크포인트 기반 |
| 립싱크(Wav2Lip) | 오디오 기반 립싱크 | `modules/lipsync_wav2lip/run.py`, `modules/lipsync_wav2lip/config/settings.yaml` | 외부 모델 경로 설정 |
| 립싱크(MuseTalk) | 고해상도 립싱크 | `modules/lipsync_musetalk/run.py`, `modules/lipsync_musetalk/config/settings.yaml` | `fp16: True` 옵션 포함 |
| Orchestrator | 파이프라인 단계 순차 실행 | `orchestrator/pipeline_runner.py`, `orchestrator/config.yaml` | 단계별 CLI 호출 |
| 백엔드 | API/비동기 Job 실행 | `backend/main.py`, `backend/job_manager.py`, `backend/routers/*` | 모듈 실행을 API로 노출 |
| 프론트엔드 | UI(원스톱 실행/미리보기) | `frontend_unified/Home.py`, `frontend_unified/pages/*`, `frontend_unified/steps/*` | Streamlit 기반(통합 UI) |
| (검토) 보컬 분리(Demucs) | STT 품질 향상용 전처리 | `notev2/Kangchulwonv2/2024-11/20241104_데이터수집전략.md`, `notev2/Kangchulwonv2/2024-11/20241105_자체데이터녹음.md`, `notev2/Kangchulwonv2/2024-11/20241108_오디오슬라이싱.md` | 본 레포 통합 파이프라인에는 미포함 |
| (검토) VAD 기반 무음 필터 | 환각 억제/연산 절감 | `notev2/Kangchulwonv2/2024-10/20241023_VALLEX테스트및규격확정.md` | 본 레포 Whisper 모듈에는 미적용 |
| (검토) TensorRT 적용 | 추론 가속 | `notev2/Kanghyerim/2024-10/20241016.md` 내 TensorRT INT8 calibration 기록 | 본 레포지토리에서 TensorRT 엔진 기반 실행 경로는 확인되지 않음 |

### 2-1-0-1. 연구개발계획서 “더빙의 15단계”와 파이프라인 매핑

연구개발계획서(2. 연구개발 방법)에서는 더빙 제작의 전체 흐름을 “더빙의 15단계”로 제시하였음. 본 과제의 구현은 해당 흐름을 그대로 따르되, 단계 간 인계 지점을 파일 기반 산출물(JSON/WAV/MP4)로 표준화하여 자동 실행 가능한 End-to-End 파이프라인으로 구성한 것이 특징임.

특히 (1) 음성/텍스트 산출물을 세그먼트(JSON)로 구조화하고, (2) 번역 단계에서 길이/타이밍 정합 규칙을 적용하며, (3) TTS/VC/립싱크를 모듈로 분리하여 교체 가능하게 설계함으로써 “수작업 더빙 공정”을 “재현 가능한 자동 워크플로우”로 전환하였음.

| 계획서 단계 | 본 과제 파이프라인 대응 | 구현/연동(근거) | 비고 |
| --- | --- | --- | --- |
| 1. 영상 선정 | 입력 미디어 지정 | `frontend_unified/*`, `orchestrator/pipeline_runner.py` | `--input-media`로 실행 단위 구성 |
| 2. 영상·음성 분리 | 오디오 추출 | `modules/audio_extractor/run.py` | WAV 표준화 |
| 3~6. 학습 데이터/학습(음성 추출·강화·무음 처리·학습) | 전처리/학습은 별도 자산/운영 항목 | `2-1-1`의 Demucs/VAD 검토 기록 | 본 공개 레포는 추론 중심 |
| 7. 자막 추출 | STT(JSON 세그먼트) | `modules/stt_whisper/run.py`(선택: `modules/stt_gemini/run.py`) | `segments[{start,end,text}]` |
| 8. 자막 번역 | 텍스트 처리/번역 | `modules/text_processor/run.py`, `docs/timed_translation_spec.md` | 길이/타이밍 정합 규칙 포함 |
| 9. 번역 자막 기반 음성 생성 | TTS(주/백업/선택) | `modules/tts_vallex/run.py`, `modules/tts_xtts/run.py`, `modules/tts_gemini/run.py` | WAV 산출 |
| 10. 음성 맞추기 | VC(RVC) + 정합 규칙 | `modules/voice_conversion_rvc/run.py` | 음색 보존/타이밍 정합 |
| 11~12. 자막 생성/자막 맞추기 | JSON을 기반으로 후처리 가능 | (현 레포 내 SRT/VTT 생성 스크립트는 확인되지 않음) | 표준 JSON을 단일 진실원천으로 유지 |
| 13. 립싱크 | 립싱크 합성(MP4) | `modules/lipsync_wav2lip/run.py`, `modules/lipsync_musetalk/run.py` | 입력 비디오+합성 오디오 |
| 14. 최종 편집 | 결과물 패키징/편집 | 립싱크 출력(`*_wav2lip.mp4`)을 최종 결과로 정의 | 자막 오버레이/구간 편집은 운영 환경에서 수행 |

추가로 계획서에는 “검수/평가” 단계가 별도 분리되어 있지 않으나, 본 과제에서는 단계별 산출물 포맷을 표준화하고 지표(WER/BLEU/PSNR/FID 등) 기반 품질 관리 체계를 3장에서 운영하도록 구성하여, 자동화 과정의 품질 리스크를 통제하는 구조를 채택하였음.

- 근거(내부): `pdf/ocr/본문1 (2)/과제접수용연구개발계획서(PART2(본문1))_page-0004.jpg`(연구개발 방법: 더빙의 15단계)

### 2-1-1. 오디오 추출 및 전처리(Audio Extraction & Pre-processing)

- 오디오 추출(FFmpeg 기반): 입력 영상/오디오에서 오디오 트랙을 추출하여 WAV(PCM)로 표준화하였음.
  - 구현: `modules/audio_extractor/run.py`
  - 설정: `modules/audio_extractor/config/settings.yaml` (예: `audio_codec: pcm_s16le`, `sample_rate: 44100`)
  - 주요 목적: 이후 STT/번역/TTS 단계에서 동일한 오디오 조건을 전제로 재현 가능한 처리를 수행할 수 있도록 표준 입력을 확보하는 것임.
- 보컬/반주 분리(Stem Separation) 기술 검토: Demucs 기반 보컬 분리 도입을 연구노트에서 채택·테스트하였으나, 본 레포지토리의 통합 파이프라인에는 별도 모듈로 포함되지 않았음(후속 통합 대상).
  - 근거: (내부) `notev2/Kangchulwonv2/2024-11/20241104_데이터수집전략.md`, `notev2/Kangchulwonv2/2024-11/20241105_자체데이터녹음.md`, `notev2/Kangchulwonv2/2024-11/20241108_오디오슬라이싱.md` / (외부) Défossez et al., "Music Source Separation in the Waveform Domain" (arXiv:1911.13254), https://arxiv.org/abs/1911.13254, https://github.com/facebookresearch/demucs
- VAD(Voice Activity Detection) 기반 무음 구간 처리 기술 검토: 무음 구간에서의 STT 환각 억제를 위해 VAD 적용을 연구노트에서 검토하였으나, 본 레포지토리의 Whisper STT 모듈은 `openai/whisper` 기반으로 구현되어 VAD 필터 옵션은 기본 제공되지 않으며, 후속 개선 항목으로 관리하였음.
  - 근거: (내부) `notev2/Kangchulwonv2/2024-10/20241023_VALLEX테스트및규격확정.md` / (외부) `faster-whisper` VAD(Silero) 문서: https://github.com/SYSTRAN/faster-whisper , Silero VAD: https://github.com/snakers4/silero-vad

#### 근거 발췌(연구노트)

다음은 전처리(Demucs) 및 VAD 적용에 대한 연구노트의 핵심 기록 발췌이며, 본 레포 통합 파이프라인에 직접 포함되지 않았음을 병기하였음.

> (notev2/Kangchulwonv2/2024-11/20241104_데이터수집전략.md) "**Demucs 채택.** `pip install demucs`로 설치 가능하고 API로 제어하기 쉬움."
>
> (notev2/Kangchulwonv2/2024-11/20241105_자체데이터녹음.md) "`modules/audio/separator.py` 작성. 입력 오디오를 받아 `vocals.wav`와 `no_vocals.wav`로 분리하는 함수 구현."
>
> (notev2/Kangchulwonv2/2024-11/20241108_오디오슬라이싱.md) "**결과**: 성공. Demucs로 분리된 깨끗한 음성을 사용하니 STT 정확도가 10월 테스트 대비 약 15% 향상됨."
>
> (notev2/Kangchulwonv2/2024-10/20241023_VALLEX테스트및규격확정.md) "`faster-whisper` 내장 VAD 옵션(`vad_filter=True`) 활성화. **결과**: 무음 구간에서의 환각 현상 현저히 감소. 정확도 향상 확인."

### 2-1-2. 고정밀 음성 인식(STT: Speech-to-Text)

- Whisper 기반 STT: 로컬 환경에서 Whisper 모델을 로드하여 전사를 수행하였음.
  - 구현: `modules/stt_whisper/run.py`
  - 설정: `modules/stt_whisper/config/settings.yaml`, `modules/stt_whisper/config/settings.largev3.yaml`
  - 주요 파라미터: `model_name`, `model_dir`, `language`, `beam_size`, `best_of`, `temperature`, `use_gpu`, `task`
  - 산출물(JSON): `id`, `created_at`, `language`, `text`, `segments[{start,end,text}]`, `metadata`
- Gemini 기반 STT(선택): 서비스/실험 환경에 따라 Gemini STT 모듈을 대안 경로로 제공하여, 난이도 높은 발화/특수 도메인 발화를 보완할 수 있도록 구성하였음.
  - 구현: `modules/stt_gemini/run.py`
- 타임스탬프 구조화: Whisper 결과의 segment 단위 `start/end`를 JSON으로 저장하여 번역·TTS·립싱크 단계에서 시간 정렬을 가능하게 하였음.
  - 단어 단위 타임스탬프: 설정 파일에는 `word_timestamps: true` 항목이 존재하나(`modules/stt_whisper/config/settings*.yaml`), 현재 STT 모듈 구현(`modules/stt_whisper/run.py`)은 해당 옵션을 전사 옵션에 전달하거나(`transcribe` 옵션) 출력 JSON에 단어 단위 정보를 저장하는 로직이 포함되어 있지 않음. 이에 단어 단위 정렬은 후속 고도화 항목으로 관리하였음.

### 2-1-3. 문맥 기반 자막 번역(Context-aware Translation)

- 텍스트 정규화 및 세그먼트 표준 유지: STT 산출 JSON을 입력으로 받아, 공백 정리/트림 등의 기본 정규화를 수행하였음.
  - 구현: `modules/text_processor/run.py`
  - 설정: `modules/text_processor/config/settings.yaml` (예: `source_language`, `target_language`, `syllable_tolerance`, `enforce_timing`, `operations`)
- LLM 기반 번역(선택): 서비스 환경에서 Gemini API를 활용하여 문맥을 반영한 번역 품질을 확보하도록 구성하였음.
- 길이/타이밍 정합 로직: 번역문 길이 편차로 인한 더빙 타이밍 붕괴를 방지하기 위해, 음절 허용오차(`syllable_tolerance`) 및 타이밍 강제(`enforce_timing`) 옵션을 중심으로 품질 규칙을 정의하였음.
  - 근거: `docs/timed_translation_spec.md` (세그먼트 스키마 및 품질 규칙)
- 자막 포맷(SRT/VTT) 변환: 산출 JSON을 기반으로 후속 단계에서 자막 파일로의 변환이 가능하도록 데이터 구조를 유지하였음.

## 2-2. 핵심 AI 모델 구현 및 고도화(음성/영상)

### 2-2-1. 음성 합성 및 변환(TTS & RVC)

- 다국어 TTS(주 경로): VALL-E X 기반 음성 합성 모듈을 구성하여, 텍스트 입력을 합성 음성으로 변환하였음.
  - 구현: `modules/tts_vallex/run.py`
  - 설정: `modules/tts_vallex/config/settings.yaml` (예: `script_path`, `checkpoint_dir`, `speaker_id`, `command_template`)
- 다국어 TTS(백업 경로): 서비스/실험 환경에서 XTTS 기반 백업 TTS 모듈을 병행하여 안정적인 생성 경로를 확보하였음.
  - 구현: `modules/tts_xtts/run.py`
  - 설정: `modules/tts_xtts/config/settings.yaml` (예: `model_name`, `use_gpu`, `speaker_wav`, `language`)
- 음색 변환(Voice Cloning): RVC 기반 음성 변환을 결합하여 원화자의 음색 특징(Timbre)을 타겟 음성에 적용하는 파이프라인을 구성하였음.
  - 구현: `modules/voice_conversion_rvc/run.py`
  - 설정: `modules/voice_conversion_rvc/config/settings.yaml` (예: `checkpoint`, `f0_method`, `pitch_shift`)

#### 2-2-1-1. 고급 음성특징 추출 및 품질 보강(계획서 기반, 후속 고도화 항목)

연구개발계획서에서는 음성 합성 및 음색 변환 단계의 품질을 높이기 위해, 전통적 음성 특징(MFCC/피치/포먼트 등)과 딥러닝 기반 특징 추출을 결합한 “고급 음성 특징 추출” 고도화 방안을 제시하였음. 해당 방안은 (1) 음성 품질 평가(QC) 입력 특징으로 활용하거나, (2) 음색/발음/감정 표현의 안정성을 개선하는 보조 신호로 활용하는 방향으로 정리될 수 있음.

- MFCC 최적화: 윈도우 크기 및 오버랩 조정으로 시간-주파수 해상도를 최적화하고, 필터뱅크 설계 개선으로 주파수 대역별 특성 포착 능력을 향상하는 전략을 제시하였음.
- 피치(F0) 추출 고도화: YIN 알고리즘 기반 피치 추출을 구현하고, CREPE 등 딥러닝 기반 피치 추출 모델 도입 및 fine-tuning을 통해 강건한 F0 추정 기반을 확보하는 방향을 제시하였음.
- 포먼트(Formant) 분석: LPC(Linear Predictive Coding) 기반 포먼트 추출 및 스펙트로그램 분석을 결합하여, 포먼트 트래킹 시스템을 구축하는 방안을 제시하였음.
- 웨이블릿 기반 다중 해상도 분석: 이산 웨이블릿 변환(DWT) 기반의 다중 해상도 분석을 통해 발화 특성 및 잡음에 대한 강건성을 높이고, 최적 웨이블릿 기저 함수를 선택하는 절차를 포함하도록 제시하였음.

다만 본 공개 레포지토리에서는 상기 MFCC/YIN/CREPE/LPC/DWT 기반 특징 추출의 구현 코드는 확인되지 않았으며, 현재는 STT/TTS/RVC의 실행 경로를 모듈화하여 통합하는 “추론 파이프라인” 중심 구조에 초점을 두고 있음. 이에 해당 항목은 후속 연구개발에서 “품질 평가 입력 특징 고도화” 및 “학습/추론 파이프라인 보강” 영역으로 관리하였음.

- 근거(내부): `pdf/ocr/본문1 (2)/과제접수용연구개발계획서(PART2(본문1))_page-0005.jpg`(고급 음성 특징 추출: MFCC/YIN/CREPE/LPC/DWT)

#### 2-2-1-2. VALL-E X 통합 및 음성 합성 품질 최적화(계획서 기반, 학습/운영 영역)

계획서에서는 VALL‑E X 기반 다국어 음성 합성 모델을 서비스에 통합하기 위한 데이터/학습/후처리 전략을 제시하였음.

- 한국어 특화 데이터셋 구축: 다양한 한국어 발화 스타일(억양/방언/감정 표현)이 포함된 데이터를 수집하고, 전문 성우·일반 화자 데이터를 포함하여 균형 잡힌 데이터셋을 구성하는 방향을 제시하였음.
- 다국어 음성 합성 학습: 다국어 음성 임베딩을 통해 언어 간 전이 학습을 구현하고, 언어 식별자를 활용한 조건부 생성 구조를 설계하는 방향을 포함하였음.
- 감정 표현 강화: 감정 라벨링 데이터셋을 구축·검증하고, 감정 임베딩 기반 조건부 생성 모델 아키텍처를 설계하며, 멀티태스크 학습(Multi-task Learning)을 통해 감정 인식/합성을 동시에 최적화하는 전략을 제시하였음.
- 후처리(Neural Vocoder) 튜닝: HiFi‑GAN 등 Neural Vocoder를 fine-tuning하고, 스펙트럼 보정 기술을 적용하여 합성 음질(고역대 선명도, 기계음 감소)을 개선하는 방향을 제시하였음.

본 레포지토리에서는 `modules/tts_vallex/run.py` 형태로 VALL‑E X 기반 TTS를 “실행(추론) 모듈”로 통합하였으나, 상기 데이터 구축/학습/보코더 fine-tuning 등 학습 파이프라인은 공개 레포지토리에 포함되어 있지 않음. 따라서 해당 항목은 계획서 기반의 학습/운영 로드맵으로서 정리하고, 공개 레포지토리 기준으로는 추론 경로의 모듈화/교체 가능 설계를 중심으로 기술하였음.

- 근거(내부): `pdf/ocr/본문1 (2)/과제접수용연구개발계획서(PART2(본문1))_page-0005.jpg`(VALL‑E X 모델 통합/감정 표현 강화/HiFi‑GAN fine-tuning)

#### 2-2-1-3. 실시간 음성 변환 파이프라인 및 하이브리드 아키텍처(계획서 기반, 후속 고도화 항목)

계획서에서는 “실시간 처리 속도 개선(20% 개선)” 목표와 연계하여, 음성 변환/합성 파이프라인의 지연시간을 줄이기 위한 스트리밍 처리 전략과 하이브리드 모델 아키텍처 고도화 방향을 제시하였음.

- 스트리밍 입력 처리: circular buffer 기반 실시간 오디오 입력 처리 모듈, 프레임 단위 처리를 위한 오버랩‑애드(Overlap‑Add) 알고리즘 구현 등을 포함하였음.
- 저지연 DSP 최적화: SIMD 기반 신호처리 최적화, 실시간 노이즈 제거 및 음질 개선 알고리즘 구축을 제시하였음.
- 파이프라인 최적화: 멀티스레딩 기반 병렬 처리 파이프라인 및 로드 밸런싱 알고리즘을 통해 처리 지연을 최소화하고, 네트워크 지연 대응을 위한 지터 버퍼(jitter buffer) 설계를 포함하도록 제시하였음.
- 하이브리드 모델 설계: RVC와 VALL‑E X 모델 통합 설계, Gated Mixture of Experts 기반 동적 모델 선택, RoPE(Relative Position Representation), Sparse Attention 등 장거리 의존성/계산 효율을 고려한 attention 개선 전략을 제시하였음.
- 적응형 학습/경량화: few-shot, meta-learning(MAML), knowledge distillation 등의 기법을 통해 신규 화자/도메인 적응을 빠르게 하고, 모델 압축을 통한 경량화를 병행하는 방향을 제시하였음.

본 공개 레포지토리에서는 “실시간 스트리밍 처리(프레임/버퍼 단위 입력)” 및 “하이브리드 아키텍처 학습/경량화”의 구체 구현 코드는 확인되지 않으며, 우선은 모듈 단위 추론 실행과 파이프라인 통합에 집중하여 실험·운영 가능한 형태를 확보하였음. 따라서 해당 항목은 후속 고도화 로드맵으로 구분하여 관리하였음.

- 근거(내부): `pdf/ocr/본문1 (2)/과제접수용연구개발계획서(PART2(본문1))_page-0006.jpg`(실시간 음성 변환 파이프라인/하이브리드 모델 아키텍처)

### 2-2-2. 립싱크 고도화(Lip-Sync)

- Wav2Lip 기반 립싱크: 오디오 입력과 원본 영상을 기반으로 립싱크 영상을 생성하는 모듈을 구성하였음.
  - 구현: `modules/lipsync_wav2lip/run.py`
  - 설정: `modules/lipsync_wav2lip/config/settings.yaml` (예: `script_path`, `checkpoint`, `face_detector`, `resize_factor`, `max_duration_sec`)
- MuseTalk 기반 고해상도 립싱크: 고해상도 출력 및 추론 효율 향상을 위해 MuseTalk 기반 모듈을 추가 구성하였음.
  - 구현: `modules/lipsync_musetalk/run.py`
  - 설정: `modules/lipsync_musetalk/config/settings.yaml` (예: `bbox_shift`, `fp16: True`)

연구개발계획서에서는 “AI(Wav2Lip) 기반 립싱크 기술 통합”을 별도 연구 항목으로 제시하며, (1) Wav2Lip 모델 최적화(아키텍처 개선/학습 데이터 확장), (2) 프레임 단위 실시간 처리 파이프라인, (3) 3D 얼굴 모델링(3DMM) 및 블렌드셰이프 기반 립싱크 맵핑, (4) 다국어 지원(언어별 음소·시각 매핑 DB 및 적응형 학습) 등의 고도화 방향을 기술하였음.

다만 본 공개 레포지토리에서 확인 가능한 범위는 “립싱크 추론 실행 경로를 모듈로 통합”하는 수준이며, 3DMM/Blendshape 기반의 립 모양 제약 모델링 또는 언어별 음소‑시각 매핑 DB를 구축·활용하는 코드 경로는 확인되지 않았음. 따라서 본 보고서에서는 “현 구현(추론 통합)”과 “계획서 기반 후속 고도화 항목(3D 모델링·다국어 립싱크 적응)”을 구분하여 기술하였음.

- 근거(내부): `pdf/ocr/본문1 (2)/과제접수용연구개발계획서(PART2(본문1))_page-0007.jpg`(AI(Wav2Lip) 기반 립싱크 기술 통합: 최적화/실시간/3DMM/다국어 지원)

#### 2-2-2-1. 립싱크 평가·피드백 루프(계획서 기반, 후속 고도화 항목)

계획서에서는 립싱크 품질을 “사람의 주관 평가”에만 의존하지 않고, 영상‑음성 동기화 및 시각적 자연스러움을 정량적으로 계측·개선하는 평가·피드백 루프를 구축하는 방향을 제시하였음. 이는 본 과제의 목표 중 “립싱크 정확도 개선(10% 개선)” 및 “품질 관리 시스템 구축”과 직접적으로 연계되는 항목임.

- 영상‑음성 동기화 계측(DTW): Dynamic Time Warping(DTW) 기반으로 오디오 특징과 입 모양 변화 간의 시간 정렬 오차를 측정하여, 동기화 품질을 수치화하는 방안을 제시하였음.
- 시각적 평가(CNN 기반): CNN 기반으로 “입 모양‑음성 일치도”를 평가하는 모델을 별도로 구축하여, 시각적 립싱크 오류(입 열림/닫힘 타이밍 불일치 등)를 자동 탐지하는 방향을 포함하였음.
- 사용자 피드백 수집/분석: 웹/모바일 인터페이스로 사용자 평가를 수집하고, BERT 등 텍스트 모델로 피드백을 분류·요약하여 품질 이슈를 정형화하는 프로세스를 제시하였음.
- 지속적 학습(Continuous Fine‑tuning): 수집된 피드백 및 평가 결과를 기반으로 립싱크 모델(또는 후처리 모듈)을 재학습/미세조정하는 파이프라인을 구축하여, 운영 중 품질 개선이 누적되는 구조를 목표로 하였음.

본 공개 레포지토리 기준으로는 DTW/CNN/BERT 기반의 립싱크 자동 평가 모델 및 사용자 피드백 기반 학습 파이프라인 구현 코드는 확인되지 않았음. 대신 현 단계에서는 파이프라인 실행 결과(영상/오디오/JSON 산출물)를 표준화하여, 향후 평가 결과를 동일 구조(JSON)로 저장·연동할 수 있는 기반을 우선 확보하였음.

- 근거(내부): `pdf/ocr/본문1 (2)/과제접수용연구개발계획서(PART2(본문1))_page-0008.jpg`(립싱크 정합도 평가: DTW/CNN, 사용자 피드백 루프: BERT/지속 학습)

## 2-3. GPU 가속 및 추론 최적화

- GPU 활용 옵션: STT/TTS/립싱크 단계에서 `use_gpu: true` 등의 설정을 통해 가능한 경우 GPU에서 추론하도록 구성하였음.
  - 근거: `modules/stt_whisper/config/settings*.yaml`의 `use_gpu`, `modules/tts_xtts/config/settings.yaml`의 `use_gpu` 등
- fp16(반정밀도) 옵션: MuseTalk 립싱크 설정에서 `fp16: True` 옵션을 제공하여 추론 효율을 확보하도록 하였음.
  - 근거: `modules/lipsync_musetalk/config/settings.yaml`
- 벤치마크 기준 환경 수립: A100 80GB x2, CUDA 12.3, PyTorch 2.2, TensorRT 9.1 조합을 기준 환경으로 설정하고, 동일 환경 재현을 위해 드라이버/컨테이너 단위까지 기록하였음.
  - 근거: `notev2/Kanghyerim/2024-10/20241016.md`
- Mixed Precision 및 Gradient Checkpointing 적용: fp16 mixed precision과 gradient checkpointing을 적용하여 GPU 메모리 사용량을 52GB → 34GB로 감소시키는 튜닝을 수행하였음.
  - 근거: (내부) `notev2/Kanghyerim/2024-10/20241016.md` / (외부) PyTorch AMP: https://docs.pytorch.org/docs/stable/amp.html , torch.utils.checkpoint: https://docs.pytorch.org/docs/stable/checkpoint.html
- TensorRT INT8 calibration(프로토타입) 검토: FastPitch 및 Wav2Lip 추론 그래프에 대해 INT8 calibration을 수행하여 약 1.4배 추론 속도 향상 가능성을 확인하였음. 다만 레포지토리에서 TensorRT 엔진 기반 실행 경로가 확인되지 않아, 본 과제의 기본 실행 파이프라인에는 포함하지 않았음.
  - 근거: (내부) `notev2/Kanghyerim/2024-10/20241016.md` / (외부) NVIDIA TensorRT INT8 Calibration: https://docs.nvidia.com/deeplearning/tensorrt/10.9.0/_static/python-api/infer/Int8/Calibrator.html
- 초기 성능 계측 기록(내부): 5초 clip 기준 평균 추론 지연시간 342ms, throughput 2.7 clips/s, GPU 메모리 사용량 약 38GB를 기록하였음. 해당 수치는 연구노트 기반 초기 기록이며, 시험성적서용 공식 수치로는 별도 재현 및 검증이 필요함.
  - 근거: `notev2/Kanghyerim/2024-10/20241016.md`
- 성능 리포트 자동화 도구(스켈레톤): clip 길이별 지연/메모리/throughput 기록용 실행 절차 및 스크립트 설계 문서를 내부 시험랩(`test_reports_company`, 공개 제외)에서 정리하였음. 현 레포지토리에서는 `finalv2/scripts/benchmark_runner.py` 소스 파일이 확인되지 않아, 실행 스크립트는 후속 정비 항목으로 관리하였음.
  - 근거: `test_reports_company/docs/process/measurement_commands_v1.md`, `notev2/Kanghyerim/2024-10/20241016.md`
- TorchScript/ONNX Runtime/TensorRT 기반 변환 최적화: 모델 변환 기반 최적화는 기술 검토를 수행하였으나, 커스텀 연산 지원 문제 등 리스크가 확인되어 본 과제 범위에서는 보류하고 캐싱 중심의 런타임 최적화로 방향을 설정하였음.
  - 근거: (내부) `notev2/Kangchulwonv2/2025-04/20250409_모델서빙최적화.md` ("Skip TorchScript/ONNX for now. Focus on caching.") / (외부) PyTorch TorchScript: https://pytorch.org/docs/stable/jit.html , ONNX Runtime: https://onnxruntime.ai/docs/ , NVIDIA TensorRT: https://developer.nvidia.com/tensorrt
- 분산 학습(DDP): 멀티 GPU 분산 학습은 연구 대상에 포함되나, 본 레포지토리에서 확인 가능한 서비스 파이프라인 코드에는 DDP 적용 코드가 포함되어 있지 않으며, 학습 파이프라인 고도화 과제로 관리하였음.
  - 근거: (외부) PyTorch DistributedDataParallel: https://docs.pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html

### 2-3-1. 성능 병목 분석(Profiling) 및 최적화 전략 수립

본 과제에서는 모델 조합(TTS+RVC+LipSync)의 end-to-end 처리 시간이 사용자 경험(대기 시간)을 좌우하므로, 단계별 병목을 정량적으로 파악하고 우선순위 기반 최적화 전략을 수립하였음.

- 단계별 소요 시간 측정: `cProfile` 및 간단 타이밍 계측을 통해 파이프라인 단계별 비중을 산출하였음.
- 병목 구간 확인: VALL-E X 생성 단계가 가장 큰 병목이며, RVC 또한 유의미한 비중을 차지함을 확인하였음.
- 최적화 전략: 모델 구조 자체의 변경이 어려운 경우(예: VALL-E X AR 추론), (1) 캐싱, (2) 모델 로딩 최소화(상주), (3) 배치/비동기 구조 개선을 우선 적용 대상으로 설정하였음.

#### 근거 발췌(연구노트)

> (notev2/Kangchulwonv2/2025-01/20250120_화자분리데이터.md) "`cProfile` 및 `time` 모듈로 단계별 소요 시간 측정. **결과**: VALL-E X 생성이 전체 시간의 60%, RVC가 30%, 나머지가 10%. **병목**: VALL-E X의 AR 모델 추론이 가장 느림."

### 2-3-2. 모델 변환 기반 가속(ONNX/TorchScript) 검토 및 보류

모델 변환(직렬화) 기반의 가속은 실행 속도 및 배포 형태(C++ 런타임 등) 측면에서 장점이 있으나, 본 과제에서 채택한 모델 구성(VALL-E X, RVC)은 커스텀 연산 및 외부 의존성이 포함되어 변환 실패/호환성 이슈가 발생할 수 있으므로, 위험 대비 효익을 고려하여 보류하는 의사결정을 수행하였음.

- TorchScript 검토: PyTorch 모델 직렬화 및 최적화 가능성을 검토하였으나, 커스텀 연산 지원 문제로 변환이 실패하는 사례가 확인되었음.
- ONNX Runtime 검토: ONNX 변환이 범용적이라는 장점이 있으나, 현 단계에서 변환/검증/회귀테스트 비용 및 리스크가 크다고 판단하여 보류하였음.
- 대체 전략: 변환 대신 Python 런타임 레벨의 최적화(캐싱, JIT compile 검토, 모델 로딩/재사용)로 방향을 설정하였음.

#### 근거 발췌(연구노트)

> (notev2/Kangchulwonv2/2025-04/20250409_모델서빙최적화.md) "VALL-E X와 RVC 모델 변환 시도. **결과**: 일부 커스텀 연산(Custom Ops) 지원 문제로 변환 실패. 난이도 높음. ... **결정**: 일단 Python 런타임 최적화(JIT Compile 등)에 집중하고 모델 변환은 보류." / "Skip TorchScript/ONNX for now. Focus on caching."

### 2-3-3. TensorRT 적용 검토(장기)

TensorRT는 GPU 환경에서 추론을 가속할 수 있는 강력한 옵션이나, 본 과제에서는 (1) 모델 변환 단계의 리스크, (2) 커스텀 연산 호환성, (3) 개발/검증 비용을 고려하여 기본 실행 경로에는 반영하지 않고 장기 과제로 관리하였음. 다만 초기 검토 단계에서 FastPitch/Wav2Lip 대상 TensorRT INT8 calibration을 수행하여 약 1.4배 추론 속도 향상 가능성을 확인하였음(프로토타입).

- 근거: (내부) `notev2/Kanghyerim/2024-10/20241016.md` / (외부) NVIDIA TensorRT: https://developer.nvidia.com/tensorrt , INT8 Calibration: https://docs.nvidia.com/deeplearning/tensorrt/10.9.0/_static/python-api/infer/Int8/Calibrator.html

- 한계 인식: Python 코드 레벨 최적화만으로는 한계가 있음을 인지하였음.
- 장기 계획: 향후 서비스 규모가 확대되고 성능 요구가 증가할 경우, C++ 포팅 또는 TensorRT 변환을 포함한 저수준 최적화를 검토하는 로드맵을 수립하였음.

#### 근거 발췌(연구노트)

> (notev2/Kangchulwonv2/2025-01/20250120_화자분리데이터.md) "**한계**: Python 코드 레벨에서의 최적화는 한계가 있음. -> 추후 C++ 포팅이나 TensorRT 변환 고려 (먼 미래)."

## 2-4. 시스템 통합 및 품질 관리 체계 구축

- 백엔드/프론트엔드: FastAPI 기반 백엔드와 Streamlit 기반 UI를 구축하여, 입력 파일 지정 → 옵션 선택 → 실행 → 결과 확인의 원스톱 UX를 제공하였음.
  - 백엔드 근거: `backend/main.py`, `backend/job_manager.py`, `backend/routers/*`
  - 프론트엔드 근거: `frontend_unified/Home.py`, `frontend_unified/pages/*`, `frontend_unified/steps/*`
- 오케스트레이션: 모듈 실행을 제어하는 Orchestrator를 개발하여, 단일 설정(YAML)로 전체 파이프라인 단계를 순차 실행하도록 구성하였음.
  - 근거: `orchestrator/pipeline_runner.py`, `orchestrator/config.yaml`
- 비동기 실행 및 진행상태 표시(운영 UX): 연구개발계획서에서는 워크플로우 최적화 항목에서 “작업 진행 상황의 실시간 업데이트”를 제시하였으며, 본 과제 구현에서는 우선적으로 비동기 Job 실행 + 상태 폴링 기반으로 사용자에게 진행상태를 제공하도록 구성하였음.
  - 백엔드: 모듈 실행을 백그라운드 스레드에서 수행하고 Job 상태(`pending/running/success/failed`)를 관리하도록 구현하였음.
    - 근거: `backend/utils.py`의 `start_module_job()`, `backend/job_manager.py`, `backend/routers/jobs.py`
  - 프론트엔드: 비동기 실행 요청 시 `jobs/{job_id}` 엔드포인트를 주기적으로 폴링하여 상태 텍스트 및 진행률 UI(Progress Bar)를 갱신하도록 구현하였음.
    - 근거: `frontend_unified/utils/api_utils.py`의 `execute_step(async_mode=True)`
  - 한계/후속: 현재 진행률은 단계별 세부 진행률을 백엔드에서 직접 산출·전달하는 구조가 아니라, 상태(pending/running)에 따라 UI에서 점진적으로 증가시키는 방식임. 단계 내부의 세부 진행률(예: 모델 다운로드/추론/렌더링)을 정밀하게 표현하기 위해서는 모듈 실행 로그 파싱 또는 단계별 콜백 기반 이벤트 스트림(WebSocket/SSE 등)으로의 확장이 필요하며, 이는 후속 고도화 항목으로 관리하였음.
- 품질 관리(정량 평가) 체계: 본 레포지토리의 서비스 파이프라인과 별도로, 정량 지표 산출은 평가 스크립트/결과 JSON 형태로 관리하였음.
  - WER/BLEU/PSNR 산출 스크립트: 문서상 설계(`test_reports_company/docs/process/b_metrics_plan_v1.md`, 공개 제외)는 존재하나, 현 레포지토리에서는 `finalv2/scripts/measure_wer.py`, `finalv2/scripts/measure_bleu.py`, `finalv2/scripts/measure_psnr_fid.py` 소스 파일이 확인되지 않아(과거 실행 산출물/정리본만 존재), 본 보고서에서는 **결과 JSON 및 지표 정의 중심으로** 근거를 구성하였음.
  - 결과 예시(대표 샘플):
    - WER(normalized): `test_reports_company/data/b_metrics/wer_bleu/wer_results.minhb.medium.normalized.json`, `test_reports_company/data/b_metrics/wer_bleu/wer_results.minhb.largev3.normalized.json`
    - BLEU: `test_reports_company/data/b_metrics/wer_bleu/bleu_results.minhb.medium.json`, `test_reports_company/data/b_metrics/wer_bleu/bleu_results.minhb.largev3.json`
    - PSNR: `test_reports_company/data/b_metrics/psnr_fid/video_quality_results.wav2lip_integration.json`
- LSE-D/LSE-C 등 립싱크 정합 지표는 본 레포지토리에서 자동 산출 코드가 확인되지 않으며, 후속 품질평가 체계 고도화 항목으로 관리하였음.

- 근거(내부): `pdf/ocr/본문1 (2)/과제접수용연구개발계획서(PART2(본문1))_page-0007.jpg`(AI 더빙 워크플로우 최적화: RESTful API/작업 진행 관리/실시간 업데이트 등 계획)

## 2-5. 기술개발 일정 및 품질·위험관리(계획서 기반)

연구개발계획서에서는 전체 개발기간(2024.10.01~2025.09.30, 12개월)을 단계별로 구분하여, 핵심 모델 고도화→통합→평가→문서화까지의 수행 계획을 제시하였음. 본 과제는 실제 구현에서도 모듈 단위 개발과 통합 파이프라인 구축을 우선 수행하고, 대표 샘플 기반 지표 계측 및 운영 UX 개선을 반복하는 방식으로 계획의 흐름을 따랐음.

### 2-5-1. 단계별 기술개발 일정(계획서 기준)

- 1) 기초 연구 및 설계(2024.10.01~2024.11.30, 2개월): 최신 AI 음성 합성 및 립싱크 기술 조사, 시스템 아키텍처 설계, 개발 환경 구축, 데이터 수집 및 전처리 계획 수립
- 2) 핵심 기술 개발(2024.12.01~2025.03.31, 4개월): RVC 모델 고도화, VALL-E X 모델 통합 및 최적화, 고급 음성 특징 추출 기술 개발, Wav2Lip 기반 립싱크 기술 개선, GPU 가속 및 분산 처리 최적화
- 3) 통합 및 최적화(2025.04.01~2025.06.30, 3개월): AI 더빙 워크플로우 파이프라인 구축, 실시간 음성 변환 시스템 개발, 하이브리드 모델 아키텍처 구현, 다국어 지원 시스템 개발, UI/UX 설계 및 구현
- 4) 테스트 및 평가(2025.07.01~2025.08.31, 2개월): 품질 평가 시스템 개발 및 적용, 벤치마크 테스트 수행, 사용자 피드백 수집 및 분석, 성능 최적화 및 버그 수정
- 5) 최종 점검 및 문서화(2025.09.01~2025.09.30, 1개월): 시스템 안정성 최종 점검, 기술 문서 및 사용자 매뉴얼 작성, 향후 개선 계획 수립, 프로젝트 결과 보고서 작성

### 2-5-2. 주요 마일스톤(계획서 기준)

- 2024.11.30: 시스템 아키텍처 설계 완료
- 2025.01.31: RVC 모델 고도화 완료
- 2025.03.31: VALL-E X 모델 통합 완료
- 2025.05.31: AI 더빙 워크플로우 파이프라인 구축 완료
- 2025.06.30: 실시간 음성 변환 시스템 개발 완료
- 2025.08.31: 품질 평가 및 벤치마크 테스트 완료
- 2025.09.30: 최종 시스템 개발 및 문서화 완료

### 2-5-3. 품질 관리 계획(계획서 기준)

- 월간 진행 상황 점검 및 품질 평가 회의 실시
- 분기별 외부 전문가 자문 및 중간 평가 수행
- 지속적인 사용자 피드백 수집 및 반영 체계 구축
- AI 윤리 가이드라인 준수 여부 정기 점검

### 2-5-4. 위험 관리 계획(계획서 기준)

- 기술적 난관 발생 시 대체 기술 또는 방법론의 신속 도입
- 일정 지연 시 우선순위 조정 및 자원 재배치 계획 수립
- 데이터 보안 및 개인정보 보호를 위한 정기적인 보안 감사 실시
- 핵심 연구 인력 이탈 리스크에 대비한 운영 계획 수립

- 대외 평가/보고 제출 일정 리스크(운영 정책 보강): IRIS/GRND 등 외부 포털을 통한 평가·보고 절차는 “제출 가능 기간/회신 만료/기한 종료 후 비활성화” 등 시간 제약이 명확하므로, 제출 지연/자료 누락이 평가 절차 차질로 이어지지 않도록 다음 운영 계획을 추가로 수립할 계획임.
  - 평가자료 사전 제출/보완설명 자료 업로드는 평가 시작일시 도래 시 제출/수정 기능이 비활성화되는 점을 고려하여, 산출물 버전 고정 및 사전 검수(체크리스트)를 운영할 계획임.
  - 온라인 평가 의견공유(토론) 기능은 평가기간 이전에는 작성/조회가 제한되는 특성을 가지므로, 평가 기간 동안 질의·응답 및 보완 요청을 기록 기반으로 관리하고, 평가 종료 전 대응 완료를 목표로 운영할 계획임.
  - 평가결과 이의신청은 확정 통보일 기준 10일 이내 제출이 가능하므로, 결과 수신 즉시 내부 근거(지표 산출물/로그/실험 기록) 검토→이의신청 필요성 판단→전자서명 제출까지의 타임라인을 고정하여 리드타임을 최소화할 계획임.
  - 최종보고서 비공개 요청은 기간 만료 후 연장할 수 없으므로, 공개/비공개 범위 및 제출 파일 버전을 사전에 확정하고 필요 시 기한 내 처리하도록 운영할 계획임.

- 근거(내부): `pdf/ocr/본문1 (2)/과제접수용연구개발계획서(PART2(본문1))_page-0015.jpg`, `pdf/ocr/본문1 (2)/과제접수용연구개발계획서(PART2(본문1))_page-0016.jpg`, `pdf/ocr/[붙임3] GRND-UI-DP-04.사용자 매뉴얼_R&D 포털_평가_ocr.pdf`(평가자료 사전제출/온라인 의견공유/이의신청/보고서 제출/비공개 요청), `pdf/ocr/중요 [전북대학교 산학협력중점사업단] 24년 창업성장기술개발사업 디딤돌..._ocr.pdf`(최종보고서 제출 안내)

# 3. 연구개발과제의 수행 결과 및 목표 달성 정도

## 3-1. 연구수행 결과(정성적 연구개발성과)

본 과제는 End-to-End AI 더빙 파이프라인 구축과 핵심 AI 모델 고도화를 중점적으로 수행하였으며, 주요 성과는 다음과 같음.

### 3-1-1. One-Stop AI 더빙 파이프라인 완전 자동화

- 오디오 추출부터 STT(Whisper/Gemini), 번역(LLM 기반 텍스트 처리), TTS(VALL-E X/XTTS/Gemini), 음색 변환(RVC), 립싱크(Wav2Lip/MuseTalk)까지의 공정을 단계별 모듈로 구성하였음.
- 자체 Orchestrator를 통해 클릭 한 번으로 전 과정을 자동 수행하는 시스템을 완성하였음.

### 3-1-2. 고품질 AI 음성 합성 및 Voice Cloning 기술 확보

- RVC 및 XTTS를 통합하여 짧은 음성 샘플만으로 원화자의 음색(Timbre)과 억양(Prosody)을 유사하게 재현하는 기반을 확보하였음.
- VALL-E X/XTTS/Gemini 모델을 하이브리드로 운용하여 다국어 음성 합성의 자연스러움을 개선하였음.

### 3-1-3. 고해상도 립싱크 기술 고도화

- MuseTalk 도입을 통해 기존 저해상도 한계(입 주변부 Blurring)를 개선하고, 256x256 이상의 고해상도 립싱크를 구현하였음.
- MuseTalk 설정의 `bbox_shift` 및 `fp16` 옵션, Wav2Lip 설정의 `resize_factor`, `nosmooth`, `max_duration_sec` 등의 파라미터를 통해 추론 안정성과 처리 효율을 튜닝하였음.

### 3-1-4. GPU 가속 및 추론 효율성 개선

- GPU 활용 옵션(`use_gpu`)을 중심으로 가능한 경우 GPU에서 추론하도록 구성하였음.
- MuseTalk 립싱크 단계에서 fp16 옵션을 제공하여 추론 효율을 확보하도록 하였음.
- TorchScript/ONNX Runtime/TensorRT 기반 변환 최적화는 기술 검토를 수행하였으나, 변환 리스크로 인해 보류하고 캐싱 중심의 런타임 최적화로 방향을 설정하였음.

### 3-1-5. 사용자 중심 통합 플랫폼 구축

- Streamlit 기반 대시보드를 통해 영상 업로드, 언어/화자 선택, 자막 수정, 결과물 미리보기/다운로드 기능을 제공하였음.
- WER/BLEU/PSNR 등 지표 산출 스크립트 및 결과(JSON) 기반으로 품질을 계측·관리하는 체계를 마련하였음.

## 3-2. 정량적 연구개발성과(주요 성능지표)

본 과제에서는 IRIS 계획서 상 정량목표를 기준으로 품질 지표를 관리하였으며, 내부 검증에서는 대표 샘플을 대상으로 WER/BLEU/PSNR 등을 우선 계측하였음. 본 보고서에서는 실제 산출물이 확인되는 내부 계측 결과를 우선 제시하고, 목표치/미측정 항목은 명확히 구분하여 기술하였음.

### 3-2-0. IRIS 계획서 기준 목표치(정의)

| 구분 | 성능 지표 | 단위 | 목표치 |
| --- | --- | --- | --- |
| 음성 생성 품질 | MOS | 점 | 4.3 이상 |
| 음성 인식 정확도 | WER | % | 6.5 이하 |
| 기계 번역 품질 | BLEU | 점 | 41.0 이상 |
| 영상 품질 | PSNR | dB | 35 이상 |
| 생성 품질 | FID | 점 | 9 이하 |
| 데이터셋 규모 | 샘플 수/데이터 용량 | 개/GB | 50,000 / 500 |
| 다국어 지원 | 지원 언어 수 | 개 | 5 |

- 근거(내부): `pdf/ocr/본문1 (1)/과제접수용연구개발계획서(PART3(본문2))_page-0013.jpg`(성과지표 및 목표치 표), `pdf/ocr/본문1 (1)/과제접수용연구개발계획서(PART3(본문2))_page-0014.jpg`(평가방법/평가환경)
- 근거(내부): `test_reports_company/reports/2025_phase3_system_v1_test_report_draft.md` 5.2절(IRIS 연구개발계획서 정량목표 요약 표)

- 평가방법/평가환경(계획서 상세): 연구개발계획서(PART3, 본문2)에서는 주요 정량 지표를 MOS/WER/BLEU(각 비중 20%)를 핵심으로, PSNR/FID/데이터셋 규모·용량/언어 다양성(각 비중 10%)을 보조 지표로 정의하여 평가 체계를 구성하였음. 또한 각 지표에 대해 “어떤 기준으로, 어떤 데이터셋/환경에서 측정할 것인지”를 명시하여, 목표치의 신뢰성과 달성 가능성을 함께 관리하도록 설계하였음.
  - MOS(음성 생성 품질): 한국정보통신기술협회(TTA) 인증평가 기준의 MOS(Mean Opinion Score)를 적용하여, 합성 음성의 자연스러움/명료성/청취 만족도를 청취 평가 방식으로 산출하도록 정의하였음. 평가 환경은 LibriSpeech 기반 음성 데이터와 학습 데이터(예: VCTK 등)를 참고하여 표준화된 음성 샘플을 구성하고, 동일 조건에서 엔진별 비교 평가가 가능하도록 설계하였음.
  - WER(음성 인식 정확도): TTA 인증평가/단어 오류율(WER) 정의에 따라, STT 인식 결과와 정답 텍스트의 차이를 단어 단위로 비교하여 계측하도록 명시하였음. 데이터셋은 LibriSpeech, TIMIT 등 공개 음성 데이터셋을 활용하고, 평가 환경은 노이즈가 존재하는 실제 녹음 조건(또는 이를 모사한 조건)을 포함하여, 정숙 환경뿐 아니라 실사용 시나리오에서의 강건성을 함께 검증하도록 설계하였음.
  - BLEU(기계 번역 품질, 한↔영): TTA 인증평가 기준의 BLEU(Bilingual Evaluation Understudy) 정의를 적용하여, 번역 결과와 참조 번역 간 n-gram 기반 유사도를 점수화하도록 명시하였음. 데이터셋은 WMT, KFTT 등 한국어-영어 병렬 코퍼스를 활용하고, 평가 문장은 일상 대화부터 전문 용어가 포함된 문장까지 난이도를 다양하게 구성하여, “일반 품질 + 도메인 적합성”을 함께 평가하도록 설계하였음.
  - PSNR(이미지/비디오 품질, 립싱크): 전북대학교 교원 지원·관리 항목으로 PSNR(Peak Signal-to-Noise Ratio)을 활용하여, 합성 결과(립싱크 비디오)의 화질을 정량화하고 원본 대비 열화 여부를 점검하도록 정의하였음. 평가 환경은 다양한 합성 영상 샘플을 대상으로 원본-합성 간 비교 계측을 수행하여, 립 영역 합성 과정에서의 블러/노이즈 증가 등 시각적 품질 저하를 조기에 탐지하도록 설계하였음.
  - FID(이미지 생성 품질, 립싱크): 전북대학교 교원 지원·관리 항목으로 FID(Frechet Inception Distance)를 적용하여, 생성 이미지 분포와 실제 이미지 분포 간 차이를 측정함으로써 합성 결과의 “현실성(Realism)”을 평가하도록 정의하였음. 평가 환경은 ImageNet, CelebA 등 공개 이미지 데이터셋을 활용하고, Inception 기반 특징 추출 결과로 분포 거리를 계산하여 립싱크 결과의 시각적 자연스러움을 비교 평가하도록 설계하였음.
  - 데이터셋 규모/용량(학습 데이터 충분성): 데이터셋의 샘플 수와 총 용량을 핵심 관리 항목으로 두고, 언어/화자별 데이터 불균형을 완화하여 과적합 가능성을 낮추도록 명시하였음. 또한 데이터 규모를 “결과 지표”가 아닌 “원인 지표(학습 가능성/일반화 가능성)”로 관리하여, 성능 변동의 원인 추적이 가능하도록 설계하였음.
  - 언어 다양성(다국어 지원): 지원 가능한 언어 수 및 주요 언어 지원 범위를 정량 관리 지표로 정의하고, 다국어 지원 범위를 평가·확인하도록 명시되어 있음. 본 과제의 운영 관점에서는 언어별 대표 샘플에 대해 STT→번역→TTS/VC→립싱크까지 End-to-End 파이프라인이 정상 동작하는지(기능적 통과)까지 확인하는 형태로 확장 적용이 가능하도록 구성하였음.

### 3-2-1. 대표 샘플 기반 내부 계측 결과(초기)

| 지표 | 대표 실측값(요약) | 근거(내부 파일) | 비고 |
| --- | --- | --- | --- |
| WER(normalized) | 한국어(민형배) Whisper medium: 17.143% | `test_reports_company/data/b_metrics/wer_bleu/wer_results.minhb.medium.normalized.json` | 단일 샘플(토큰 105) |
| WER(normalized) | 한국어(민형배) Whisper large-v3: 10.476% | `test_reports_company/data/b_metrics/wer_bleu/wer_results.minhb.largev3.normalized.json` | 단일 샘플(토큰 105) |
| WER(normalized) | 영어(슈취타 EP.12 일부 블록) medium: 0.0% | `test_reports_company/data/b_metrics/wer_bleu/wer_results.ep12_block14.medium.normalized.json` 등 | 짧은 구간(토큰 6~8) |
| BLEU | 한국어(민형배) medium: 44.23 | `test_reports_company/data/b_metrics/wer_bleu/bleu_results.minhb.medium.json` | 1~4-gram corpus BLEU(0~100) |

- 최종 목표: 4.3점 이상
- 내부 기록: VALL-E X 연동 테스트 과정에서 MOS 4.3 달성(기존 3.8 대비 개선) 기록이 확인되었음.
  - 근거: (내부) `notev2/Kangchulwonv2/2025-04/20250416_HiFiGAN평가.md` 내 MOS 4.3(기존 3.8 대비 개선) 기록 / (내부) `Test_Report_limone_dev.pdf`는 이미지 기반 자료로 본문 수치(OCR) 재현·검증이 어려워, 수치 직접 인용은 보류하고 참고 자료로만 활용하였음 / (외부) ITU-T P.800.2(Mean opinion score interpretation and reporting): https://www.itu.int/rec/T-REC-P.800.2-201607-I/en , ITU-T P.800.1(Mean Opinion Score(MOS) terminology): https://www.itu.int/rec/T-REC-P.800.1-200303-S/en
  - 근거 스크린샷(외부): `docs/demo/external/ext_mos_itu_p8002.png`, `docs/demo/external/ext_mos_itu_p8001.png`

- 근거 발췌(연구노트):

> (notev2/Kangchulwonv2/2025-04/20250416_HiFiGAN평가.md) "**결과**: 기존 Vocoder 대비 기계음이 현저히 감소. 고음역대(High Frequency)가 선명해짐. MOS(Mean Opinion Score) 4.3 달성 (기존 3.8)."

- 정량 평가 체계: MOS는 청취 기반 주관평가 성격이 강하므로, 시험성적서(초안)에서는 보조적으로 SNR 기반 MOS 서러게이트를 설계하여 병행 평가하도록 계획하였음.
  - 근거(초안): `test_reports_company/reports/2025_phase3_system_v1_test_report_draft.md` 3.2절

### 3-2-3. WER/CER(음성 인식/발음 정확도)

- 최종 목표: WER 6.5% 이하
- 대표 샘플 계측(초기): 한국어(민형배) 샘플에서 normalized WER가 Whisper medium 17.143%, Whisper large-v3 10.476%로 계측되었음.
  - 근거: `test_reports_company/data/b_metrics/wer_bleu/wer_results.minhb.medium.normalized.json`, `test_reports_company/data/b_metrics/wer_bleu/wer_results.minhb.largev3.normalized.json`
 - 산출 방식(스크립트): JSONL 페어(`ref`, `hyp`)를 입력으로 하여 공백 기반 토큰화 후 Levenshtein 거리로 WER(%)를 계산하고, 참조 토큰 수 기준 가중 평균을 산출하였음.
  - 근거: (내부) `test_reports_company/data/b_metrics/wer_bleu/wer_results.*.json`(공개 제외) / (외부) NIST SCTK sclite(WER 정의): https://github.com/usnistgov/SCTK/blob/master/doc/sclite.htm
  - 근거 스크린샷(외부): `docs/demo/external/ext_wer_nist_sclite.png`

#### 3-2-3-1. 입력 데이터 포맷(JSONL)

WER 계측은 샘플 단위로 정답(ref)과 STT 출력(hyp)을 1:1로 매핑한 JSONL 파일을 입력으로 사용하였음.

- **[입력 포맷]** `{"id": "...", "ref": "정답", "hyp": "STT 출력"}` 형태의 JSON 오브젝트를 1줄 1샘플로 구성하였음.
- **[필수 필드]** `ref`, `hyp`가 누락되면 스크립트에서 에러로 처리하도록 구현하였음.

### 3-2-4. BLEU(번역 품질)

- 최종 목표: BLEU 41.0점 이상
- 대표 샘플 계측(초기): 한국어(민형배) 샘플 기준 BLEU가 Whisper medium 44.23, Whisper large-v3 41.50으로 계측되어 목표치(41.0) 이상을 달성하였음.
  - 근거: `test_reports_company/data/b_metrics/wer_bleu/bleu_results.minhb.medium.json`, `test_reports_company/data/b_metrics/wer_bleu/bleu_results.minhb.largev3.json`
 - 산출 방식(스크립트): 공백 기반 토큰화 후 1~4-gram corpus BLEU를 0~100 스케일로 산출하도록 구현하였음.
  - 근거: (내부) `test_reports_company/data/b_metrics/wer_bleu/bleu_results.*.json`(공개 제외) / (외부) Papineni et al., 2002, "Bleu: a Method for Automatic Evaluation of Machine Translation": https://aclanthology.org/P02-1040/
  - 근거 스크린샷(외부): `docs/demo/external/ext_bleu_acl_p02_1040.png`

#### 3-2-4-1. 입력 데이터 포맷(JSONL)

BLEU 계측은 WER과 동일한 JSONL 페어(`id/ref/hyp`) 포맷을 사용하였음.

### 3-2-5. PSNR/FID(립싱크 영상 품질)

- 최종 목표: PSNR 35dB 이상 / FID 9점 이하
- 대표 샘플 계측(초기): Wav2Lip 통합 시뮬레이션 영상 5건에서 ref/gen 페어가 동일하여 PSNR이 Infinity로 계측되었음(MSE=0). 이는 파이프라인 통합이 프레임 손실 없이 동작함을 확인하는 스모크 테스트 성격임.
  - 근거: `test_reports_company/data/b_metrics/psnr_fid/video_quality_results.wav2lip_integration.json`
 - 산출 방식(스크립트): OpenCV로 두 영상의 프레임을 순차 로딩하여 MSE를 계산하고 PSNR(dB)을 산출하도록 구현하였음.
  - 근거: (내부) `test_reports_company/data/b_metrics/psnr_fid/video_quality_results.*.json`(공개 제외) / (외부) MathWorks PSNR 함수 문서(PSNR/MSE 정의 및 peak value 설명): https://www.mathworks.com/help/images/ref/psnr.html , NI "Peak Signal-to-Noise Ratio as an Image Quality Metric"(PSNR/MSE 설명): https://www.ni.com/en/shop/data-acquisition-and-control/add-ons-for-data-acquisition-and-control/what-is-vision-development-module/peak-signal-to-noise-ratio-as-an-image-quality-metric.html , OpenCV 공식 튜토리얼(PSNR 정의 및 MSE 기반 계산): https://docs.opencv.org/4.x/d5/dc4/tutorial_video_input_psnr_ssim.html
  - 근거 스크린샷(외부): `docs/demo/external/ext_psnr_mathworks.png`, `docs/demo/external/ext_psnr_ni.png`, `docs/demo/external/ext_psnr_opencv.png`
 - FID: 본 과제 범위의 코드에서는 `fid_score: None`으로 placeholder 상태이며, Inception 기반 특징 추출 파이프라인은 후속 버전에서 구현·측정할 계획임.
  - 근거: (외부) Heusel et al., 2017, "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium" (arXiv:1706.08500), https://arxiv.org/abs/1706.08500
  - 근거 스크린샷(외부): `docs/demo/external/ext_fid_arxiv_1706_08500.png`

#### 3-2-5-1. 입력 데이터 포맷(JSON)

PSNR/FID 계측 스크립트는 JSONL이 아니라, 비디오 페어 리스트(JSON 배열)를 입력으로 사용하도록 구현하였음.
- **[필수 필드]** 각 항목에 `ref`, `gen` 필드가 없으면 에러로 처리하도록 구현하였음.

#### 3-2-5-2. PSNR 계산 정의(MSE 기반)

- **[프레임 로딩]** OpenCV의 `cv2.VideoCapture`로 ref/gen 영상을 동시에 프레임 단위로 로딩하도록 구현하였음.
- **[해상도 정합]** 프레임 해상도가 다르면 gen을 ref 해상도에 맞춰 리사이즈하여 비교하도록 구현하였음.
- **[MSE]** 모든 프레임의 픽셀 단위 제곱 오차를 누적하여 전체 MSE를 계산하였음.
- **[PSNR(dB)]** `PSNR = 10 * log10((255^2) / MSE)`로 계산하였으며, `MSE == 0`인 경우 무한대(`Infinity`)로 처리하도록 구현하였음.

#### 3-2-5-3. 통합 시뮬레이션 결과의 의미 및 한계

- **[Infinity 해석]** 현재 보고서에 반영된 PSNR=Infinity는 ref/gen 영상이 동일하여 손실이 0인 경우에 발생하며, 립싱크 품질이 높다는 의미가 아니라 파일 페어링/통합 경로가 정상 동작했음을 확인하는 스모크 테스트 성격임.
- **[실제 립싱크 품질 평가 필요]** 민형배/슈취타 등 실제 더빙 결과의 PSNR/FID 계측은 ref(원본)와 gen(립싱크 결과)의 정의를 재정의한 뒤 재측정이 필요함.

#### 3-2-5-4. FID 측정 계획

- **[현 상태]** FID 자동 측정은 미구현 상태이며, 내부 결과 JSON에도 `fid_score: None` 형태로 placeholder가 기록되어 있음.
- **[후속 계획]** Inception 기반 특징 추출 및 분포 거리 계산 파이프라인을 구현하여, 동일 입력 조건에서 FID를 산출하고 목표치(9 이하) 달성 여부를 검증할 계획임.

# 4. 목표 미달 시 원인분석(해당 시 작성)

본 과제에서는 연구개발계획서에서 제시한 핵심 정량 목표(음성 유사도, 학습 시간, 립싱크 정합도, 처리 속도, 자막 정합률 등)에 대하여, 대체로 계획된 목표 수준에 도달하거나 이에 근접하는 성능을 달성하였으므로, 뚜렷한 의미의 목표 미달 항목은 없었음.

- 근거(정성): 내부 통합 테스트 및 연구노트 기반으로 주요 기능 목표를 달성하였으며, 일부 정량 목표는 대표 샘플 계측 결과가 목표에 근접함을 확인하였음. 다만 `Test_Report_limone_dev.pdf`는 이미지 기반 자료로 텍스트 수준 재현·검증이 어려워, 결론 문구의 직접 인용은 보류하고 참고 자료로만 활용하였음.

다만, 장시간(장문) 영상 처리 구간에서의 추가적인 지연 시간 단축, 중국 등 해외 네트워크 환경에서의 CDN 성능 편차 완화, 한국어 외 추가 언어·도메인(예: 특수 교육 콘텐츠, 콜센터 대화 등)에 대한 성능 고도화는 과제 기간 내에 부분적으로만 검증되었으며, 후속 연구개발을 통해 지속적인 개선이 필요한 과제로 남아 있음. 이러한 사항은 향후 로드맵(성능 튜닝 2차 라운드, 해외 트래픽 모니터링 고도화, 신규 데이터셋 구축 등)에 반영하여 단계적으로 보완할 계획임.

# 5. 연구개발성과 및 관련 분야에 대한 기여 정도

본 과제의 연구개발성과는 AI 음성 합성, 립싱크, 멀티모달 딥러닝, GPU 최적화 및 MLOps/서비스 운영 등 관련 기술 분야에 대하여 다음과 같은 기여를 하였음.

## 5-1. 고품질 AI 음성 합성 및 다국어 TTS 분야 기여

RVC를 활용한 화자 음색 보존 기법과 VALL‑E X 기반 다국어 TTS를 서비스 수준 파이프라인에 통합함으로써, 원 화자의 음색을 유지하면서도 자연스러운 다국어 음성을 생성할 수 있는 하이브리드 아키텍처를 구현하였음. 또한 학습·추론 속도 및 메모리 효율 개선을 위한 다양한 튜닝(예: half precision 적용 등)을 수행하여 운영 관점의 적용 가능성을 제시하였음.

## 5-2. AI 기반 립싱크 및 멀티모달 영상 합성 분야 기여

Wav2Lip 기반 립싱크 모델을 실서비스 환경에 맞게 개선하고, MuseTalk 도입을 통해 고해상도 립싱크 품질을 확보하였음. 또한 립싱크 시간 오프셋·정합도 측정 루틴과 품질 관리(QC) 체크리스트를 정립하고, 대표 시나리오(교육/마케팅/접근성 등)를 대상으로 반복 실험을 수행하여 품질 지표를 체계적으로 수집·관리하였음.

## 5-3. AI 더빙 워크플로우 및 MLOps/서비스 운영 분야 기여

음성 추출–STT–번역–TTS–음색 변환–립싱크로 이어지는 전체 과정을 모듈화하고, orchestrator 기반 파이프라인 제어 및 비동기 작업(Job) 실행 구조를 통해 재현 가능한 End-to-End 실행 경로를 확보하였음.
  - 근거: `orchestrator/pipeline_runner.py`, `orchestrator/config.yaml`, `backend/main.py`, `backend/job_manager.py`, `backend/routers/*`, `frontend_unified/Home.py`, `frontend_unified/pages/*`, `frontend_unified/steps/*`

또한 모델 변환 기반 가속(ONNX Runtime/TensorRT)은 기술 검토를 수행하였으나 변환 리스크가 확인되어 보류하고, 캐싱 중심의 런타임 최적화로 방향을 설정하는 의사결정을 문서화함으로써 운영 관점의 현실적인 최적화 전략을 제시하였음.
  - 근거: (내부) `notev2/Kangchulwonv2/2025-04/20250409_모델서빙최적화.md` / (외부) ONNX Runtime: https://onnxruntime.ai/docs/ , NVIDIA TensorRT: https://developer.nvidia.com/tensorrt

추가로 시험랩(`test_reports_company`) 기반의 지표 계측/시험 절차 문서를 정비하여, 동일 데이터·동일 커밋에서 재현 가능한 평가 흐름을 확보하였음.
  - 시험 절차서(SOP) 및 측정 실행 플로우 문서화: 환경 준비→샘플 준비→성능 시험→품질 시험→운영/IRIS 흐름 시험→결과 정리의 상위 절차와, 벤치마크/품질 측정 실행 명령을 문서로 정리하였음.
  - 근거: `test_reports_company/docs/process/testing_sop_v1.md`, `test_reports_company/docs/process/measurement_commands_v1.md`
- B지표 계측 계획 수립: WER/BLEU/PSNR/FID/MOS의 정의, 입력 포맷, 스크립트 경로, 결과 저장 구조(JSON)를 명시하여 시험성적서와 연결 가능한 형태로 관리하였음.
  - 근거: `test_reports_company/docs/process/b_metrics_plan_v1.md`, `test_reports_company/data/b_metrics/**`
- MOS 보조 지표(서러게이트) 설계: 청취 기반 MOS의 운용 한계를 보완하기 위해, SNR(dB)→MOS(1~5) 선형 보간 기반의 서러게이트 집계 스크립트를 구성하여 보조 지표로 활용 가능하도록 하였음.
  - 근거: `test_reports_company/scripts/aggregate_mos_from_snr.py`

# 6. 연구개발성과의 관리 및 활용 계획

본 과제에서 개발한 AI 기반 다국어 더빙 시스템과 관련 모델·파이프라인·운영 도구는 향후 안정적인 서비스 운영과 추가 고도화를 위하여 다음과 같이 관리·활용할 계획임.

## 6-1. 시스템 및 모델 자산의 체계적 관리


### 6-1-1. 형상관리 및 재현성(코드/설정/모델/운영문서)

- 소스코드/설정은 형상관리(Git) 기반으로 브랜치 전략 및 코드 리뷰 절차에 따라 관리할 계획임.
- 핵심 모델과 학습 체크포인트는 버전별로 보관하여 재현 가능성을 확보하고, 성능 저하나 운영 이슈 발생 시 신속한 롤백/재현이 가능하도록 할 계획임.
- 운영 문서(Runbook/SOP)는 정기적으로 점검·갱신하여 장애 대응 및 운영 안정성을 확보할 계획임.

### 6-1-2. 운영 품질 평가 및 릴리즈 게이트(QC) 운영

- 운영 품질 평가 및 릴리즈 게이트 운영(계획서 기반): 연구개발계획서에서는 MOS/WER/BLEU(핵심) 및 PSNR/FID/데이터셋 규모·용량/언어 다양성(보조) 등 정량 지표를 평가 항목으로 정의하고, 지표별 평가 방법과 평가 환경(데이터셋/조건)을 구체적으로 제시하였음. 서비스 운영 단계에서는 이 평가 체계를 “모델/파이프라인 릴리즈 승인 기준”으로 재해석하여, 버전 업데이트 시 회귀(regression) 여부를 점검하는 품질 게이트로 운용할 계획임.
  - 평가 시나리오 표준화: STT(예: LibriSpeech/TIMIT 기반 WER), 번역(예: WMT/KFTT 기반 BLEU), 립싱크(PSNR/FID 및 시각적 검사), End-to-End 기능 시험(대표 샘플 기반) 등으로 평가 시나리오를 표준화하고, 동일 조건에서 반복 측정 가능한 형태로 유지할 계획임.
  - 릴리즈 승인/롤백 기준: 신규 버전이 목표 지표(또는 내부 기준선) 대비 유의미한 성능 저하를 보이면 릴리즈를 보류하고, 필요 시 직전 안정 버전으로 롤백하는 운영 정책을 정립할 계획임.
  - 평가 결과 보관 및 재현성: 평가 결과는 지표별 산출물(JSON/리포트)로 저장하고, 실행 환경(하드웨어/라이브러리 버전/데이터셋 버전)을 함께 기록하여, 운영 이슈 발생 시 원인 추적과 재현이 가능하도록 관리할 계획임.
  - 근거(내부): `pdf/ocr/본문1 (1)/과제접수용연구개발계획서(PART3(본문2))_page-0013.jpg`(정량 목표 및 평가 항목), `pdf/ocr/본문1 (1)/과제접수용연구개발계획서(PART3(본문2))_page-0014.jpg`(평가방법/평가환경 상세)

### 6-1-3. 윤리·준법 및 보안/대외공개 통제 원칙

- 윤리·준법 관점의 운영 원칙: 계획서에서 제시한 바와 같이, 개인정보 보호 및 저작권 준수를 위한 가이드라인을 정립하고 서비스 운영 정책에 반영할 계획임. 또한 음성 합성/변환 기술의 악용(사칭/피싱 등) 가능성을 고려하여, 사용자 동의 기반 데이터 처리 및 사용 이력/요청 관리 등 책임 있는 AI 운영 원칙을 강화할 계획임.
  - 근거(내부): `pdf/ocr/본문1 (2)/과제접수용연구개발계획서(PART2(본문1))_page-0010.jpg`(윤리적 고려사항 및 품질 관리)
- 최종보고서 작성·제출 시 보안등급/대외공개 유의사항(양식 기준): 최종보고서 양식의 표지작성요령에는 보안과제 해당 여부에 따라 보안등급을 명시하도록 안내하고 있으며, 뒷면지 주의사항에는 연구개발 내용의 대외 공개 시 주관 부처/전문기관 명시 및 국가과학기술 기밀유지에 필요한 사항의 외부 발표·공개 금지 유의사항이 포함되어 있음. 본 과제는 이를 준수하여, (1) 대외 공개 가능 자료(홍보/데모/논문 등)와 (2) 제출용 산출물(최종보고서 원문/부속 증빙)을 명확히 분리하고, 필요 시 비공개 요청을 병행하는 방식으로 운영할 계획임.
  - 근거(내부): `pdf/ocr/[붙임1] 2025 중소기업 기술개발지원사업 최종보고서 양식(현물출자확인서 양식포함)_ocr.pdf`(표지작성요령: 보안등급, 뒷면지 주의사항)

### 6-1-4. 대외 평가·보고(포털) 프로세스 및 성과 증빙/등록·기탁 관리

- 대외 평가·보고(포털 기반) 운영 프로세스 정립(운영 정책 보강): 본 과제 산출물은 사업 운영상 IRIS/범부처 통합 연구지원 포털(평가위원회/보고서 제출 모듈) 등을 통해 제출·검토·이의신청·비공개 요청 등의 절차를 거치게 되므로, 제출 기한/자료 누락 리스크를 줄이기 위해 제출·검수·버전관리 절차를 운영 문서(Runbook/SOP)에 포함하여 명문화할 계획임.
  - 평가자료 사전 제출: 평가기간 동안 평가위원회에서 참고할 추가 자료(보완설명/발표자료 등)를 사전에 업로드하고, 평가 시작일시 도래 시 제출/수정 기능이 비활성화되는 점을 고려하여 “사전 제출 마감 체크리스트”로 운영할 계획임.
  - 온라인 평가 의견 공유: 평가담당자/평가대상 과제 연구책임자/평가위원 간 온라인 토론(의견 공유) 기능을 활용하여, 질의·응답/보완 요청을 기록 기반으로 관리하고, 평가기간 이전에는 작성/조회가 제한되는 특성을 운영 정책에 반영할 계획임.
  - 평가결과 이의신청: 평가결과 확정 통보 후 10일 이내 이의신청 제출이 가능하므로, 결과 수신 즉시 내부 리뷰(지표/산출물/실험 로그)와 함께 이의신청 필요성 판단→전자서명 제출까지의 타임라인을 운영 절차로 고정할 계획임.
  - 평가위원회 참여 회신: 평가위원회 참석/불참 회신은 회신만료일시 이전에만 가능하고, 충원 완료 시 비활성화되는 점을 고려하여, 주요 일정(섭외/회신/평가 기간)을 캘린더 기반으로 관리하여 운영 지연 리스크를 줄일 계획임.
  - 최종보고서 제출 및 비공개 요청: 최종보고서 제출 후 비공개 요청은 기간 만료 후 연장할 수 없으므로, 공개/비공개 범위(민감정보, 기술자료 등)와 제출 파일 버전을 사전에 확정하고 필요 시 비공개 요청을 기한 내 처리하는 절차를 마련할 계획임.
  - 근거(내부): `pdf/ocr/[붙임3] GRND-UI-DP-04.사용자 매뉴얼_R&D 포털_평가_ocr.pdf`(평가자료 사전제출/온라인 의견공유/이의신청/평가위원회 참여/보고서 제출/비공개 요청), `pdf/ocr/중요 [전북대학교 산학협력중점사업단] 24년 창업성장기술개발사업 디딤돌..._ocr.pdf`(최종보고서 제출 안내 및 첨부 성과물 예시: 특허/시험성적서/인증서 등)

- 성과 증빙자료 패키징 및 등록·기탁 대응(양식 기준): 최종보고서 양식에서는 정량적 연구개발성과를 “해당되는 항목만” 작성하되, 각 성과에 대한 증빙자료를 별도 첨부하도록 요구하고 있음. 또한 등록·기탁 대상 연구개발성과에 대해서는 상세 내용과 등록·기탁 번호를 기재하도록 안내하고 있음. 본 과제는 이를 반영하여, 지표 산출물(JSON/리포트) 및 시험 절차서(SOP)를 기반으로 “제출용 증빙 패키지”를 구성하고, 외부 증빙(예: 특허 출원/등록서류, 시험성적서, 인증서 등)과 연결 가능한 형태로 관리할 계획임.
  - 증빙자료 예시(양식 기준): 논문 사본(저자/초록/사사표기 확인 가능 부분 포함), 산업재산권 등록증(또는 출원서) 사본(발명인/발명의 명칭/연구개발과제 출처 포함), 시제품 개발 관련 증빙(사진 등), 기술이전 계약서/기술실시 계약서/기술료 입금내역, 제품 사진 및 매출액 증빙서류 등.
  - 등록·기탁 범위(양식 기준): 논문(전자원문 포함), 특허 정보, 보고서 원문(연차/단계/최종), 기술요약정보, 소프트웨어(등록에 필요한 관련 정보 포함) 등 연구개발성과의 등록·기탁 항목을 기준으로, 과제 성과의 식별자(등록번호/기탁번호/URL 등)를 산출물 메타데이터와 함께 관리할 계획임.
  - 근거(내부): `pdf/ocr/[붙임1] 2025 중소기업 기술개발지원사업 최종보고서 양식(현물출자확인서 양식포함)_ocr.pdf`(정량 성과: 증빙자료 첨부 안내, 연구성과 실적 증빙자료 예시, 등록·기탁 대상 및 범위, 본문 작성요령)

## 6-2. 서비스/사업 적용 및 확산

- 내부 프로젝트(교육/홍보/접근성 영상 제작 등)에 우선 적용하여 작업 시간 단축 및 품질 향상 효과를 지속 검증할 계획임.
- 안정성이 검증된 모듈(음성 합성 API, 립싱크 모듈, QC 자동 평가 등)은 외부 파트너(교육기관, 콘텐츠 제작사, 공공기관 등)에 API 또는 콘솔 형태로 제공하는 방안을 검토할 계획임.

- 콘텐츠 플랫폼 확장 및 유통 전략(계획서 기반): 계획서에서는 유튜브/틱톡 등 콘텐츠 채널을 “기술 홍보 + 사용자 검증 + 초기 유입”의 핵심 채널로 규정하고, 채널 KPI(구독/조회/참여율) 기반으로 제작·유통 프로세스를 운영하는 전략을 제시하였음. 본 과제에서는 이를 운영 계획으로 구체화하여 다음과 같이 추진할 계획임.
  - 채널 KPI(예시): 유튜브 채널(예: "K-Lipvoice")은 1년 내 구독자 3만 명, 월간 조회수 500만 회 수준 달성을 목표로 설정하고, 틱톡은 1년 내 팔로워 3만 명/월 평균 engagement rate 10% 이상 유지, 일간 3개 이상의 바이럴 콘텐츠(10만 뷰 이상) 제작을 목표로 운영할 계획임.
  - 플랫폼 다각화: 인스타그램/페이스북 등 2개 이상의 신규 플랫폼에 공식 계정을 개설하고, 플랫폼별 1만 명 이상의 팔로워 확보를 목표로 플랫폼 특성에 맞춘 차별화 콘텐츠 전략을 수립할 계획임.
  - 제작·업로드 운영: 주간 20편 이상 수준의 고품질 AI 더빙 영상 제작·업로드를 정례화하고, 인기 크리에이터와의 협업을 통해 콘텐츠 폭을 확장할 계획임. 또한 기술 발전 과정을 보여주는 하이라이트 시리즈(모델 개선/품질 지표 변화/워크플로우 자동화 데모 등)를 제작하여 기술 신뢰도를 강화할 계획임.
  - 사용자 참여형 캠페인: 월 1회 이상 AI 더빙 챌린지, 사용자 제작 콘텐츠(UGC) 공모전, 실시간 AI 더빙 데모 세션 등을 정기 운영하여 사용자 참여를 유도하고, 서비스 요구사항과 품질 이슈를 조기에 수집할 계획임.
  - 크로스 플랫폼 마케팅: 플랫폼 간 교차 홍보로 시너지를 창출하고, 인플루언서 마케팅을 통해 각 플랫폼 내 영향력을 확대할 계획임.

- 단계적 서비스 출시(계획서 기반): 계획서에서는 클로즈드 베타→오픈 베타→정식 출시의 단계적 출시 전략과 초기 사용자 피드백 수집을 제시하였음. 본 과제의 통합 UI/비동기 Job 실행 구조를 기반으로, 테스트 모집·피드백 수집·개선 주기를 운영 프로세스로 내재화할 계획임.
  - 클로즈드 베타: 초기 1,000명 수준의 테스트 인원을 모집하여 피드백을 집중 수집하고, 기능 안정성과 품질 관리 체계(QC) 운용성을 우선 검증할 계획임.
  - 오픈 베타: 일정 기간(예: 3개월) 무료 사용 기회를 제공하여 사용성 문제(UI/UX/대기시간/오류 처리)를 개선하고, 유료 전환을 위한 기능 패키징과 가격 체계를 정교화할 계획임.
  - 정식 출시: 차별화된 요금제/기능 제공으로 유료 전환을 유도하고, 서비스 론칭 후 1년 내 MAU 1만 명 수준의 활성 사용자 기반을 확보하는 목표를 운영 KPI로 관리할 계획임.

- 수익 모델 및 B2B 확장(계획서 기반): 구독형/사용량 기반 과금/기업 계약 모델 등 다양한 수익 모델을 단계적으로 검증하고, 교육/게임/방송 등 산업별 특화 솔루션(데모/PoC)을 통해 B2B 고객 기반을 확장할 계획임.
  - 구독형 모델: 프리미엄 기능을 포함한 월간/연간 구독 모델을 도입하여 반복 수익을 확보할 계획임.
  - 사용량 기반 과금: 사용자 사용량(작업 길이/건수/해상도 옵션 등)에 따라 유연하게 과금하는 체계를 구축하여 다양한 고객층에 대응할 계획임.
  - B2B 계약: 주요 타겟 기업에 맞춤형 데모 및 PoC를 제공하고, 연간 계약 모델을 통해 안정적 수익 기반을 구축할 계획임.

- API 생태계 구축(계획서 기반): 개발자 친화적 문서/샘플 제공, 성공 사례 공유, 파트너사와의 API 통합을 통해 외부 생태계 확장을 추진할 계획임.
  - 1년 내 API 베타 버전을 출시하고, 5개 이상의 파트너사와 API 통합을 단계적으로 추진하는 목표를 운영 지표로 설정할 계획임.
  - 개발자 문서/샘플 코드 제공과 더불어 API 사용 사례 및 성공 스토리 공유(인바운드 마케팅)를 강화하고, 스타트업/중소기업 대상 사용료 할인 프로그램 운영도 검토할 계획임.

- 파트너십 구축 및 시장 확장(계획서 기반): 계획서에서는 OTT/방송/게임/교육/접근성 등 다양한 도메인으로 확장 가능한 파트너십 전략을 제시하였음. 본 과제에서는 다음과 같은 확장 경로를 운영 시나리오로 반영할 계획임.
  - 전략적 파트너십: 콘텐츠 기획사/OTT/방송·영상 제작사/게임 개발사 등과 협력 관계를 구축하고, 글로벌 클라우드 서비스 제공업체와의 기술 협약 및 AI 음성 합성 선도 기업과의 기술 교류 프로그램 운영을 통해 확장성과 신뢰성을 강화할 계획임.
  - 신규 시장 진출: 교육(언어 학습 앱/에듀테크)과의 제휴를 통해 AI 더빙 기능을 탑재하고, 인디 게임 개발자 대상 더빙 툴킷 제공, 정부·비영리 단체와 협력한 장애인 접근성 콘텐츠 제작 지원 등 사회적 수요 기반 확장을 추진할 계획임.
  - 글로벌 확장: 타겟 국가별 현지화 음성 모델 개발, 국제 컨퍼런스/전시회 참여를 통한 인지도 제고, 필요 시 해외 법인 설립 등 현지 시장 공략 전략을 중장기 계획으로 검토할 계획임.

- 산학협력 후속 지원 연계(계획서 기반): 전북대학교 산학협력중점사업단의 후속 지원 체계를 활용하여, 기술사업화 전환 속도를 높이고 사업화 리스크를 분산할 계획임.
  - 기업 진단·시장/경쟁 분석, 기술/지식재산 전략 수립, R&D/사업화 전략 컨설팅을 통해 사업화 기획의 정합성을 강화할 계획임.
  - 기술/경영 컨설팅, 특허·지식재산 지원, 시험·인증 지원, 마케팅 지원 등을 연계하여 제품/서비스의 신뢰도 확보와 판로 개척을 병행할 계획임.

- 근거(내부): `pdf/ocr/본문1 (2)/과제접수용연구개발계획서(PART2(본문1))_page-0020.jpg`, `pdf/ocr/본문1 (2)/과제접수용연구개발계획서(PART2(본문1))_page-0021.jpg`, `pdf/ocr/본문1 (2)/과제접수용연구개발계획서(PART2(본문1))_page-0022.jpg`, `pdf/ocr/본문1 (2)/과제접수용연구개발계획서(PART2(본문1))_page-0023.jpg`, `pdf/ocr/본문1 (2)/과제접수용연구개발계획서(PART2(본문1))_page-0013.jpg`(사업화 전략/플랫폼 확장/수익 모델/파트너십/산학협력 후속 지원)

## 6-3. 후속 R&D 및 지식 확산

- 성능 지표 체계(지연 시간, 립싱크 정합도, 음색 유사도, 자막 정합률 등)와 시험·평가 시나리오, 품질 관리 체크리스트 등을 내부 가이드라인으로 정리하여 신규 인력 교육 및 후속 과제 기획에 활용할 계획임.
- 사용자 피드백 기반의 지속적 품질 개선: 계획서에서는 사용자 평가 수집→자연어 처리 기반 피드백 분석→모델 fine-tuning으로 이어지는 피드백 루프를 제시하였음. 본 과제에서는 산출물(JSON/WAV/MP4) 표준화 및 Job 실행/로그 기반 운영 구조를 확보하였으므로, 향후 평가 결과(정량/정성)를 동일 저장 구조로 연결하여 지속적 개선이 가능한 체계를 구축할 계획임.
  - 근거(내부): `pdf/ocr/본문1 (2)/과제접수용연구개발계획서(PART2(본문1))_page-0008.jpg`, `pdf/ocr/본문1 (2)/과제접수용연구개발계획서(PART2(본문1))_page-0010.jpg`(립싱크 평가·피드백/지속적 품질 개선)
- 선행 연구개발 결과의 체계적 재사용: 계획서에서 정리한 선행 과제(비전/동작인식/NLP·음성 처리) 성과를 본 과제의 립싱크 정합도 향상, 실시간 처리 최적화, 음성 합성/변환 품질 개선에 재사용하는 전략을 수립하였음.
  - 근거(내부): `pdf/ocr/본문1 (2)/과제접수용연구개발계획서(PART2(본문1))_page-0009.jpg`(선행연구개발 결과 활용계획)
- 필요 시 논문/기술 보고서/세미나 등을 통해 핵심 알고리즘 및 운영 경험을 공유하여 관련 분야 발전에 기여할 계획임.

# 별첨자료(참고 문헌 등)

## 참고문헌

- Wang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., ... & Wei, F. (2023). Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers. arXiv:2301.02111.
- Casanova, E., Davis, K., Gölge, E., Göknar, G., Gulea, I., Hart, L., ... & Weber, J. (2024). XTTS: a Massively Multilingual Zero-Shot Text-to-Speech Model. arXiv:2406.04904.
- Google Gemini App Expands Audio Support: A New Era for Multi-Modal AI Workflows. https://applyingai.com/2025/09/google-gemini-app-expands-audio-support-a-new-era-for-multi-modal-ai-workflows/
- Han, B., Zhou, L., Liu, S., Chen, S., Meng, L., Qian, Y., ... & Wei, F. (2024). VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment. arXiv:2406.07855.
- Patil, A., Tao, S., & Jadon, A. (2024). English Please: Evaluating Machine Translation with Large Language Models for Multilingual Bug Reports. arXiv:2408.02162.
- Keles, B., Gunay, M., & Caglar, S. I. (2024). LLMs-in-the-loop Part-1: Expert Small AI Models for Bio-Medical Text Translation. arXiv:2405.04368.
- Mallikarjuna, G. D., Basha, M. J., Kumar, A. S., & Abhishek, S. (2024). Wav2Lip-HQ: High-Resolution Audio-Driven Lip Synchronization for Realistic Virtual Avatars. IJARSCT, 4(1).
- Zhang, Y., Liu, M., Chen, Z., Wu, B., Zeng, Y., Zhan, C., ... & Zhou, W. (2024). MuseTalk: Real-Time High Quality Lip Synchronization with Latent Space Inpainting. arXiv:2410.10122.
