# 최종보고서(본문)

- 연구개발과제명: AI 기반 다국어 음성 합성 및 실시간 립싱크 더빙 시스템 개발
- 연구개발과제번호: RS-2024-00511307
- 연구개발기간: 2024.10.01 ~ 2025.09.30

## 목차

- 1. 연구개발과제의 개요
- 2. 연구개발과제의 수행 과정 및 수행내용
- 3. 연구개발과제의 수행 결과 및 목표 달성 정도
- 4. 목표 미달 시 원인분석(해당 시 작성)
- 5. 연구개발성과 및 관련 분야에 대한 기여 정도
- 6. 연구개발성과의 관리 및 활용 계획
- 별첨자료(참고 문헌 등)

# 1. 연구개발과제의 개요

## 1-1. 개발 배경 및 필요성

본 연구개발과제는 중소벤처기업부 창업성장기술개발사업(디딤돌)의 일환으로 추진된 과제로서, AI 기반 다국어 음성 합성 및 실시간 립싱크 더빙 시스템을 이용하여 영상 콘텐츠의 다국어 더빙을 One-Stop으로 자동 처리할 수 있는 플랫폼을 구축하고자 하였음.

최근 OTT 및 유튜브 등 글로벌 콘텐츠 시장이 급성장함에 따라 다국어 더빙 수요가 증가하고 있으나, 기존 더빙 제작 방식은 언어별 스크립트 작성, 전문 성우 섭외 및 녹음, 영상 편집 및 입 모양(Lip-sync) 싱크 작업을 단계별 수작업으로 처리해야 하는 구조적 한계가 있음. 이로 인해 제작 비용이 높고 제작 기간이 길어, 자금력과 인력이 제한된 중소기업 및 1인 창작자가 글로벌 시장에 진출하는 데 큰 진입 장벽으로 작용하였음.

이에 본 과제에서는 최신 AI 기술을 융합하여 번역부터 음성 합성, 립싱크까지 전 과정을 자동화함으로써, 기존 대비 비용과 시간을 절감하고 누구나 쉽게 고품질 다국어 콘텐츠를 제작할 수 있는 솔루션을 개발하고자 하였음.

## 1-2. 최종 목표

과제의 세부 목표는 연구개발계획서에서 제시한 바와 같이 다음 4가지를 핵심 지표로 설정하였음.

- 고품질 AI 음성 합성 기술 개발
  - 원화자의 음색과 감정(억양/발화 습관)을 보존하면서도 자연스러운 다국어 음성을 생성
- AI 기반 립싱크 기술 고도화
  - 합성된 음성에 맞춰 영상 속 인물의 입 모양을 정교하게 동기화
- AI 더빙 워크플로우 최적화
  - 음성 추출부터 렌더링까지의 전체 공정을 파이프라인화하여 작업 효율을 극대화
- AI 더빙 품질 관리 시스템 개발
  - 결과물 품질을 정량적으로 측정(WER/CER, LSE-D/LSE-C 등)하고 피드백할 수 있는 체계를 구축

## 1-3. 주요 연구 내용 및 차별성

본 과제는 단순히 여러 모델을 조합하는 수준을 넘어, 영상 입력부터 최종 더빙 영상 산출까지의 흐름을 재현 가능한 모듈로 구조화하고 운영 가능한 형태로 통합한 점에 차별성이 있음.

### 1-3-1. End-to-End One-Stop 자동화 프로세스 설계

- 영상 입력부터 최종 편집(렌더링/QC 포함)까지 전체 공정을 세분화하여 자동화 가능한 형태로 설계하였음.
- 모듈 간 데이터 표준(JSON 기반 타임스탬프/자막 포맷 등)을 정의하여 단계별 산출물이 다음 단계에 자연스럽게 이어지도록 구성하였음.

### 1-3-2. 하이브리드 AI 모델 활용(단계별 최적 모델 모듈화)

- STT: Whisper 기반 로컬 STT를 기본으로 하고, 난이도 높은 발화/전문 용어의 경우 Gemini API를 선택적으로 활용하는 이중화 체계를 구성하였음.
- TTS: VALL-E X, XTTS 등 다국어 TTS 모델을 적용하고, 서비스 환경에 맞는 튜닝/운용 전략을 수립하였음.
- Lip-Sync: Wav2Lip 기반 립싱크 파이프라인에 더하여 MuseTalk을 추가 도입하여 고해상도 립싱크 품질을 확보하였음.

### 1-3-3. 화자 음색 보존(Voice Cloning)

RVC(Retrieval-based Voice Conversion) 기반 Voice Cloning 기술을 도입하여, 단순히 언어만 바꾸는 것이 아니라 원작자의 목소리 톤과 감정을 유지한 채 다른 언어로 말하는 듯한 고품질 더빙을 구현하고자 하였음.

### 1-3-4. 고성능/고화질 립싱크 파이프라인

기존 Wav2Lip 계열 모델의 한계를 보완하기 위해 MuseTalk을 추가 도입하였으며, GPU 활용 및 fp16 옵션(예: MuseTalk 설정의 `fp16: True`)을 통해 고화질 영상에서도 처리 효율을 확보하고자 하였음.

또한 TorchScript/ONNX Runtime 기반 모델 변환 및 TensorRT 적용은 기술 검토를 수행하였으나, 커스텀 연산 호환성 및 변환 리스크로 인해 본 과제 범위에서는 보류하고 Python 런타임 최적화(캐싱 등)에 집중하는 전략을 수립하였음.

## 1-4. 추진 경과 및 기대 효과

본 과제는 2024.10.01.~2025.09.30. 기간 동안 주관기관((주)파디엠)과 공동연구기관(전북대학교 산학협력단)의 협력을 통해 수행되었음. 연구개발계획서 상 기술성숙도(TRL) 9단계 달성을 목표로 파이프라인과 핵심 모델을 단계적으로 고도화하였음.

기대 효과는 다음과 같이 정리할 수 있음.

- 기술적 효과
  - STT/번역/TTS/립싱크 등 분절된 공정을 통합하여 고품질 더빙 제작의 진입 장벽을 낮추고 접근성을 제고
  - Voice Cloning 및 고화질 립싱크 기술을 통해 글로벌 수준의 더빙 품질 기반을 확보
- 경제적 효과
  - 수작업 대비 제작 비용과 제작 기간을 절감하여, 중소 제작사/1인 창작자의 해외 시장 진출을 지원
- 사회적 효과
  - 교육/접근성(장애인·고령자 지원) 등 정보 접근성 향상에 기여
- 장기적 효과
  - 메타버스/가상인간 등 차세대 미디어 분야로 확장 가능한 핵심 기반 기술 확보

# 2. 연구개발과제의 수행 과정 및 수행내용

## 2-1. 전체 파이프라인 설계 및 워크플로우 최적화

본 연구개발과제는 연구개발계획서에서 제시한 연차·단계별 추진계획에 따라, (1) 고품질 AI 음성 합성, (2) AI 기반 립싱크 고도화, (3) 워크플로우 최적화, (4) 품질 관리 시스템 개발의 4대 목표를 중심으로 수행되었음.

### 2-1-0. 파이프라인 모듈 구성 및 근거(코드 경로)

본 과제의 End-to-End 파이프라인은 단계별로 독립 실행 가능한 모듈(`modules/*/run.py`)로 구현하고, (1) Orchestrator에서 순차 실행하거나 (2) FastAPI 백엔드 API에서 모듈을 호출하도록 구성하였음.

다음 표는 본 보고서에서 기술하는 파이프라인 단계와 실제 코드/설정 파일의 대응 관계를 요약한 것으로, 본문 서술의 근거로 활용하였음.

| 구분 | 목적 | 구현/연동(근거) | 비고 |
| --- | --- | --- | --- |
| 오디오 추출 | 영상/오디오 입력에서 표준 WAV 추출 | `modules/audio_extractor/run.py`, `modules/audio_extractor/config/settings.yaml` | FFmpeg 경로/코덱/샘플레이트 설정 기반 |
| STT(Whisper) | 음성→텍스트 전사 | `modules/stt_whisper/run.py`, `modules/stt_whisper/config/settings*.yaml` | `openai/whisper` 기반 |
| STT(Gemini, 선택) | 난이도 높은 발화 보완용 STT | `modules/stt_gemini/run.py` | API 기반(선택 경로) |
| 텍스트 처리/번역 | 정규화·번역·길이/타이밍 정합 | `modules/text_processor/run.py`, `modules/text_processor/config/settings.yaml`, `docs/timed_translation_spec.md` | JSON 세그먼트 표준을 유지 |
| TTS(VALL-E X) | 텍스트→음성 합성(주) | `modules/tts_vallex/run.py`, `modules/tts_vallex/config/settings.yaml` | 외부 스크립트 실행 형태 |
| TTS(XTTS, 백업) | 텍스트→음성 합성(보조) | `modules/tts_xtts/run.py`, `modules/tts_xtts/config/settings.yaml` | 백업 경로 |
| TTS(Gemini, 선택) | API 기반 음성 합성 | `modules/tts_gemini/run.py` | 선택 경로 |
| 음색 변환(RVC) | 원화자 음색 유지(VC) | `modules/voice_conversion_rvc/run.py`, `modules/voice_conversion_rvc/config/settings.yaml` | 체크포인트 기반 |
| 립싱크(Wav2Lip) | 오디오 기반 립싱크 | `modules/lipsync_wav2lip/run.py`, `modules/lipsync_wav2lip/config/settings.yaml` | 외부 모델 경로 설정 |
| 립싱크(MuseTalk) | 고해상도 립싱크 | `modules/lipsync_musetalk/run.py`, `modules/lipsync_musetalk/config/settings.yaml` | `fp16: True` 옵션 포함 |
| Orchestrator | 파이프라인 단계 순차 실행 | `orchestrator/pipeline_runner.py`, `orchestrator/config.yaml` | 단계별 CLI 호출 |
| 백엔드 | API/비동기 Job 실행 | `backend/main.py`, `backend/job_manager.py`, `backend/routers/*` | 모듈 실행을 API로 노출 |
| 프론트엔드 | UI(원스톱 실행/미리보기) | `frontend_unified/Home.py`, `frontend_unified/pages/*`, `frontend_unified/steps/*` | Streamlit 기반(통합 UI) |
| (검토) 보컬 분리(Demucs) | STT 품질 향상용 전처리 | `notev2/Kangchulwonv2/2024-11/20241104_데이터수집전략.md`, `notev2/Kangchulwonv2/2024-11/20241105_자체데이터녹음.md`, `notev2/Kangchulwonv2/2024-11/20241108_오디오슬라이싱.md` | 본 레포 통합 파이프라인에는 미포함 |
| (검토) VAD 기반 무음 필터 | 환각 억제/연산 절감 | `notev2/Kangchulwonv2/2024-10/20241023_VALLEX테스트및규격확정.md` | 본 레포 Whisper 모듈에는 미적용 |
| (검토) TensorRT 적용 | 추론 가속 | `notev2/Kanghyerim/2024-10/20241016.md` 내 TensorRT INT8 calibration 기록 | 본 레포지토리에서 TensorRT 엔진 기반 실행 경로는 확인되지 않음 |

### 2-1-1. 오디오 추출 및 전처리(Audio Extraction & Pre-processing)

- 오디오 추출(FFmpeg 기반): 입력 영상/오디오에서 오디오 트랙을 추출하여 WAV(PCM)로 표준화하였음.
  - 구현: `modules/audio_extractor/run.py`
  - 설정: `modules/audio_extractor/config/settings.yaml` (예: `audio_codec: pcm_s16le`, `sample_rate: 44100`)
  - 주요 목적: 이후 STT/번역/TTS 단계에서 동일한 오디오 조건을 전제로 재현 가능한 처리를 수행할 수 있도록 표준 입력을 확보하는 것임.
- 보컬/반주 분리(Stem Separation) 기술 검토: Demucs 기반 보컬 분리 도입을 연구노트에서 채택·테스트하였으나, 본 레포지토리의 통합 파이프라인에는 별도 모듈로 포함되지 않았음(후속 통합 대상).
  - 근거: (내부) `notev2/Kangchulwonv2/2024-11/20241104_데이터수집전략.md`, `notev2/Kangchulwonv2/2024-11/20241105_자체데이터녹음.md`, `notev2/Kangchulwonv2/2024-11/20241108_오디오슬라이싱.md` / (외부) Défossez et al., "Music Source Separation in the Waveform Domain" (arXiv:1911.13254), https://arxiv.org/abs/1911.13254, https://github.com/facebookresearch/demucs
- VAD(Voice Activity Detection) 기반 무음 구간 처리 기술 검토: 무음 구간에서의 STT 환각 억제를 위해 VAD 적용을 연구노트에서 검토하였으나, 본 레포지토리의 Whisper STT 모듈은 `openai/whisper` 기반으로 구현되어 VAD 필터 옵션은 기본 제공되지 않으며, 후속 개선 항목으로 관리하였음.
  - 근거: (내부) `notev2/Kangchulwonv2/2024-10/20241023_VALLEX테스트및규격확정.md` / (외부) `faster-whisper` VAD(Silero) 문서: https://github.com/SYSTRAN/faster-whisper , Silero VAD: https://github.com/snakers4/silero-vad

#### 근거 발췌(연구노트)

다음은 전처리(Demucs) 및 VAD 적용에 대한 연구노트의 핵심 기록 발췌이며, 본 레포 통합 파이프라인에 직접 포함되지 않았음을 병기하였음.

> (notev2/Kangchulwonv2/2024-11/20241104_데이터수집전략.md) "**Demucs 채택.** `pip install demucs`로 설치 가능하고 API로 제어하기 쉬움."
>
> (notev2/Kangchulwonv2/2024-11/20241105_자체데이터녹음.md) "`modules/audio/separator.py` 작성. 입력 오디오를 받아 `vocals.wav`와 `no_vocals.wav`로 분리하는 함수 구현."
>
> (notev2/Kangchulwonv2/2024-11/20241108_오디오슬라이싱.md) "**결과**: 성공. Demucs로 분리된 깨끗한 음성을 사용하니 STT 정확도가 10월 테스트 대비 약 15% 향상됨."
>
> (notev2/Kangchulwonv2/2024-10/20241023_VALLEX테스트및규격확정.md) "`faster-whisper` 내장 VAD 옵션(`vad_filter=True`) 활성화. **결과**: 무음 구간에서의 환각 현상 현저히 감소. 정확도 향상 확인."

### 2-1-2. 고정밀 음성 인식(STT: Speech-to-Text)

- Whisper 기반 STT: 로컬 환경에서 Whisper 모델을 로드하여 전사를 수행하였음.
  - 구현: `modules/stt_whisper/run.py`
  - 설정: `modules/stt_whisper/config/settings.yaml`, `modules/stt_whisper/config/settings.largev3.yaml`
  - 주요 파라미터: `model_name`, `model_dir`, `language`, `beam_size`, `best_of`, `temperature`, `use_gpu`, `task`
  - 산출물(JSON): `id`, `created_at`, `language`, `text`, `segments[{start,end,text}]`, `metadata`
- Gemini 기반 STT(선택): 서비스/실험 환경에 따라 Gemini STT 모듈을 대안 경로로 제공하여, 난이도 높은 발화/특수 도메인 발화를 보완할 수 있도록 구성하였음.
  - 구현: `modules/stt_gemini/run.py`
- 타임스탬프 구조화: Whisper 결과의 segment 단위 `start/end`를 JSON으로 저장하여 번역·TTS·립싱크 단계에서 시간 정렬을 가능하게 하였음.
  - 단어 단위 타임스탬프: 설정 파일에는 `word_timestamps: true` 항목이 존재하나(`modules/stt_whisper/config/settings*.yaml`), 현재 STT 모듈 구현(`modules/stt_whisper/run.py`)은 해당 옵션을 전사 옵션에 전달하거나(`transcribe` 옵션) 출력 JSON에 단어 단위 정보를 저장하는 로직이 포함되어 있지 않음. 이에 단어 단위 정렬은 후속 고도화 항목으로 관리하였음.

### 2-1-3. 문맥 기반 자막 번역(Context-aware Translation)

- 텍스트 정규화 및 세그먼트 표준 유지: STT 산출 JSON을 입력으로 받아, 공백 정리/트림 등의 기본 정규화를 수행하였음.
  - 구현: `modules/text_processor/run.py`
  - 설정: `modules/text_processor/config/settings.yaml` (예: `source_language`, `target_language`, `syllable_tolerance`, `enforce_timing`, `operations`)
- LLM 기반 번역(선택): 서비스 환경에서 Gemini API를 활용하여 문맥을 반영한 번역 품질을 확보하도록 구성하였음.
- 길이/타이밍 정합 로직: 번역문 길이 편차로 인한 더빙 타이밍 붕괴를 방지하기 위해, 음절 허용오차(`syllable_tolerance`) 및 타이밍 강제(`enforce_timing`) 옵션을 중심으로 품질 규칙을 정의하였음.
  - 근거: `docs/timed_translation_spec.md` (세그먼트 스키마 및 품질 규칙)
- 자막 포맷(SRT/VTT) 변환: 산출 JSON을 기반으로 후속 단계에서 자막 파일로의 변환이 가능하도록 데이터 구조를 유지하였음.

## 2-2. 핵심 AI 모델 구현 및 고도화(음성/영상)

### 2-2-1. 음성 합성 및 변환(TTS & RVC)

- 다국어 TTS(주 경로): VALL-E X 기반 음성 합성 모듈을 구성하여, 텍스트 입력을 합성 음성으로 변환하였음.
  - 구현: `modules/tts_vallex/run.py`
  - 설정: `modules/tts_vallex/config/settings.yaml` (예: `script_path`, `checkpoint_dir`, `speaker_id`, `command_template`)
- 다국어 TTS(백업 경로): 서비스/실험 환경에서 XTTS 기반 백업 TTS 모듈을 병행하여 안정적인 생성 경로를 확보하였음.
  - 구현: `modules/tts_xtts/run.py`
  - 설정: `modules/tts_xtts/config/settings.yaml` (예: `model_name`, `use_gpu`, `speaker_wav`, `language`)
- 음색 변환(Voice Cloning): RVC 기반 음성 변환을 결합하여 원화자의 음색 특징(Timbre)을 타겟 음성에 적용하는 파이프라인을 구성하였음.
  - 구현: `modules/voice_conversion_rvc/run.py`
  - 설정: `modules/voice_conversion_rvc/config/settings.yaml` (예: `checkpoint`, `f0_method`, `pitch_shift`)

### 2-2-2. 립싱크 고도화(Lip-Sync)

- Wav2Lip 기반 립싱크: 오디오 입력과 원본 영상을 기반으로 립싱크 영상을 생성하는 모듈을 구성하였음.
  - 구현: `modules/lipsync_wav2lip/run.py`
  - 설정: `modules/lipsync_wav2lip/config/settings.yaml` (예: `script_path`, `checkpoint`, `face_detector`, `resize_factor`, `max_duration_sec`)
- MuseTalk 기반 고해상도 립싱크: 고해상도 출력 및 추론 효율 향상을 위해 MuseTalk 기반 모듈을 추가 구성하였음.
  - 구현: `modules/lipsync_musetalk/run.py`
  - 설정: `modules/lipsync_musetalk/config/settings.yaml` (예: `bbox_shift`, `fp16: True`)
- 립싱크 정합(평가): 립싱크 품질 정량 평가는 별도 평가 환경(시험성적서/평가 스크립트)에서 수행하도록 설계하였음.

## 2-3. GPU 가속 및 추론 최적화

- GPU 활용 옵션: STT/TTS/립싱크 단계에서 `use_gpu: true` 등의 설정을 통해 가능한 경우 GPU에서 추론하도록 구성하였음.
  - 근거: `modules/stt_whisper/config/settings*.yaml`의 `use_gpu`, `modules/tts_xtts/config/settings.yaml`의 `use_gpu` 등
- fp16(반정밀도) 옵션: MuseTalk 립싱크 설정에서 `fp16: True` 옵션을 제공하여 추론 효율을 확보하도록 하였음.
  - 근거: `modules/lipsync_musetalk/config/settings.yaml`
- 벤치마크 기준 환경 수립: A100 80GB x2, CUDA 12.3, PyTorch 2.2, TensorRT 9.1 조합을 기준 환경으로 설정하고, 동일 환경 재현을 위해 드라이버/컨테이너 단위까지 기록하였음.
  - 근거: `notev2/Kanghyerim/2024-10/20241016.md`
- Mixed Precision 및 Gradient Checkpointing 적용: fp16 mixed precision과 gradient checkpointing을 적용하여 GPU 메모리 사용량을 52GB → 34GB로 감소시키는 튜닝을 수행하였음.
  - 근거: (내부) `notev2/Kanghyerim/2024-10/20241016.md` / (외부) PyTorch AMP: https://docs.pytorch.org/docs/stable/amp.html , torch.utils.checkpoint: https://docs.pytorch.org/docs/stable/checkpoint.html
- TensorRT INT8 calibration(프로토타입) 검토: FastPitch 및 Wav2Lip 추론 그래프에 대해 INT8 calibration을 수행하여 약 1.4배 추론 속도 향상 가능성을 확인하였음. 다만 레포지토리에서 TensorRT 엔진 기반 실행 경로가 확인되지 않아, 본 과제의 기본 실행 파이프라인에는 포함하지 않았음.
  - 근거: (내부) `notev2/Kanghyerim/2024-10/20241016.md` / (외부) NVIDIA TensorRT INT8 Calibration: https://docs.nvidia.com/deeplearning/tensorrt/10.9.0/_static/python-api/infer/Int8/Calibrator.html
- 초기 성능 계측 기록(내부): 5초 clip 기준 평균 추론 지연시간 342ms, throughput 2.7 clips/s, GPU 메모리 사용량 약 38GB를 기록하였음. 해당 수치는 연구노트 기반 초기 기록이며, 시험성적서용 공식 수치로는 별도 재현 및 검증이 필요함.
  - 근거: `notev2/Kanghyerim/2024-10/20241016.md`
- 성능 리포트 자동화 도구(스켈레톤): clip 길이별 지연/메모리/throughput 기록용 실행 절차 및 스크립트 설계 문서를 내부 시험랩(`test_reports_company`, 공개 제외)에서 정리하였음. 현 레포지토리에서는 `finalv2/scripts/benchmark_runner.py` 소스 파일이 확인되지 않아, 실행 스크립트는 후속 정비 항목으로 관리하였음.
  - 근거: `test_reports_company/docs/process/measurement_commands_v1.md`, `notev2/Kanghyerim/2024-10/20241016.md`
- TorchScript/ONNX Runtime/TensorRT 기반 변환 최적화: 모델 변환 기반 최적화는 기술 검토를 수행하였으나, 커스텀 연산 지원 문제 등 리스크가 확인되어 본 과제 범위에서는 보류하고 캐싱 중심의 런타임 최적화로 방향을 설정하였음.
  - 근거: (내부) `notev2/Kangchulwonv2/2025-04/20250409_모델서빙최적화.md` ("Skip TorchScript/ONNX for now. Focus on caching.") / (외부) PyTorch TorchScript: https://pytorch.org/docs/stable/jit.html , ONNX Runtime: https://onnxruntime.ai/docs/ , NVIDIA TensorRT: https://developer.nvidia.com/tensorrt
- 분산 학습(DDP): 멀티 GPU 분산 학습은 연구 대상에 포함되나, 본 레포지토리에서 확인 가능한 서비스 파이프라인 코드에는 DDP 적용 코드가 포함되어 있지 않으며, 학습 파이프라인 고도화 과제로 관리하였음.
  - 근거: (외부) PyTorch DistributedDataParallel: https://docs.pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html

### 2-3-1. 성능 병목 분석(Profiling) 및 최적화 전략 수립

본 과제에서는 모델 조합(TTS+RVC+LipSync)의 end-to-end 처리 시간이 사용자 경험(대기 시간)을 좌우하므로, 단계별 병목을 정량적으로 파악하고 우선순위 기반 최적화 전략을 수립하였음.

- 단계별 소요 시간 측정: `cProfile` 및 간단 타이밍 계측을 통해 파이프라인 단계별 비중을 산출하였음.
- 병목 구간 확인: VALL-E X 생성 단계가 가장 큰 병목이며, RVC 또한 유의미한 비중을 차지함을 확인하였음.
- 최적화 전략: 모델 구조 자체의 변경이 어려운 경우(예: VALL-E X AR 추론), (1) 캐싱, (2) 모델 로딩 최소화(상주), (3) 배치/비동기 구조 개선을 우선 적용 대상으로 설정하였음.

#### 근거 발췌(연구노트)

> (notev2/Kangchulwonv2/2025-01/20250120_화자분리데이터.md) "`cProfile` 및 `time` 모듈로 단계별 소요 시간 측정. **결과**: VALL-E X 생성이 전체 시간의 60%, RVC가 30%, 나머지가 10%. **병목**: VALL-E X의 AR 모델 추론이 가장 느림."

### 2-3-2. 모델 변환 기반 가속(ONNX/TorchScript) 검토 및 보류

모델 변환(직렬화) 기반의 가속은 실행 속도 및 배포 형태(C++ 런타임 등) 측면에서 장점이 있으나, 본 과제에서 채택한 모델 구성(VALL-E X, RVC)은 커스텀 연산 및 외부 의존성이 포함되어 변환 실패/호환성 이슈가 발생할 수 있으므로, 위험 대비 효익을 고려하여 보류하는 의사결정을 수행하였음.

- TorchScript 검토: PyTorch 모델 직렬화 및 최적화 가능성을 검토하였으나, 커스텀 연산 지원 문제로 변환이 실패하는 사례가 확인되었음.
- ONNX Runtime 검토: ONNX 변환이 범용적이라는 장점이 있으나, 현 단계에서 변환/검증/회귀테스트 비용 및 리스크가 크다고 판단하여 보류하였음.
- 대체 전략: 변환 대신 Python 런타임 레벨의 최적화(캐싱, JIT compile 검토, 모델 로딩/재사용)로 방향을 설정하였음.

#### 근거 발췌(연구노트)

> (notev2/Kangchulwonv2/2025-04/20250409_모델서빙최적화.md) "VALL-E X와 RVC 모델 변환 시도. **결과**: 일부 커스텀 연산(Custom Ops) 지원 문제로 변환 실패. 난이도 높음. ... **결정**: 일단 Python 런타임 최적화(JIT Compile 등)에 집중하고 모델 변환은 보류." / "Skip TorchScript/ONNX for now. Focus on caching."

### 2-3-3. TensorRT 적용 검토(장기)

TensorRT는 GPU 환경에서 추론을 가속할 수 있는 강력한 옵션이나, 본 과제에서는 (1) 모델 변환 단계의 리스크, (2) 커스텀 연산 호환성, (3) 개발/검증 비용을 고려하여 기본 실행 경로에는 반영하지 않고 장기 과제로 관리하였음. 다만 초기 검토 단계에서 FastPitch/Wav2Lip 대상 TensorRT INT8 calibration을 수행하여 약 1.4배 추론 속도 향상 가능성을 확인하였음(프로토타입).

- 근거: (내부) `notev2/Kanghyerim/2024-10/20241016.md` / (외부) NVIDIA TensorRT: https://developer.nvidia.com/tensorrt , INT8 Calibration: https://docs.nvidia.com/deeplearning/tensorrt/10.9.0/_static/python-api/infer/Int8/Calibrator.html

- 한계 인식: Python 코드 레벨 최적화만으로는 한계가 있음을 인지하였음.
- 장기 계획: 향후 서비스 규모가 확대되고 성능 요구가 증가할 경우, C++ 포팅 또는 TensorRT 변환을 포함한 저수준 최적화를 검토하는 로드맵을 수립하였음.

#### 근거 발췌(연구노트)

> (notev2/Kangchulwonv2/2025-01/20250120_화자분리데이터.md) "**한계**: Python 코드 레벨에서의 최적화는 한계가 있음. -> 추후 C++ 포팅이나 TensorRT 변환 고려 (먼 미래)."

## 2-4. 시스템 통합 및 품질 관리 체계 구축

- 백엔드/프론트엔드: FastAPI 기반 백엔드와 Streamlit 기반 UI를 구축하여, 입력 파일 지정 → 옵션 선택 → 실행 → 결과 확인의 원스톱 UX를 제공하였음.
  - 백엔드 근거: `backend/main.py`, `backend/job_manager.py`, `backend/routers/*`
  - 프론트엔드 근거: `frontend_unified/Home.py`, `frontend_unified/pages/*`, `frontend_unified/steps/*`
- 오케스트레이션: 모듈 실행을 제어하는 Orchestrator를 개발하여, 단일 설정(YAML)로 전체 파이프라인 단계를 순차 실행하도록 구성하였음.
  - 근거: `orchestrator/pipeline_runner.py`, `orchestrator/config.yaml`
- 품질 관리(정량 평가) 체계: 본 레포지토리의 서비스 파이프라인과 별도로, 정량 지표 산출은 평가 스크립트/결과 JSON 형태로 관리하였음.
  - WER/BLEU/PSNR 산출 스크립트: 문서상 설계(`test_reports_company/docs/process/b_metrics_plan_v1.md`, 공개 제외)는 존재하나, 현 레포지토리에서는 `finalv2/scripts/measure_wer.py`, `finalv2/scripts/measure_bleu.py`, `finalv2/scripts/measure_psnr_fid.py` 소스 파일이 확인되지 않아(과거 실행 산출물/정리본만 존재), 본 보고서에서는 **결과 JSON 및 지표 정의 중심으로** 근거를 구성하였음.
  - 결과 예시(대표 샘플):
    - WER(normalized): `test_reports_company/data/b_metrics/wer_bleu/wer_results.minhb.medium.normalized.json`, `test_reports_company/data/b_metrics/wer_bleu/wer_results.minhb.largev3.normalized.json`
    - BLEU: `test_reports_company/data/b_metrics/wer_bleu/bleu_results.minhb.medium.json`, `test_reports_company/data/b_metrics/wer_bleu/bleu_results.minhb.largev3.json`
    - PSNR: `test_reports_company/data/b_metrics/psnr_fid/video_quality_results.wav2lip_integration.json`
- LSE-D/LSE-C 등 립싱크 정합 지표는 본 레포지토리에서 자동 산출 코드가 확인되지 않으며, 후속 품질평가 체계 고도화 항목으로 관리하였음.

# 3. 연구개발과제의 수행 결과 및 목표 달성 정도

## 3-1. 연구수행 결과(정성적 연구개발성과)

본 과제는 End-to-End AI 더빙 파이프라인 구축과 핵심 AI 모델 고도화를 중점적으로 수행하였으며, 주요 성과는 다음과 같음.

### 3-1-1. One-Stop AI 더빙 파이프라인 완전 자동화

- 오디오 추출부터 STT(Whisper/Gemini), 번역(LLM 기반 텍스트 처리), TTS(VALL-E X/XTTS/Gemini), 음색 변환(RVC), 립싱크(Wav2Lip/MuseTalk)까지의 공정을 단계별 모듈로 구성하였음.
- 자체 Orchestrator를 통해 클릭 한 번으로 전 과정을 자동 수행하는 시스템을 완성하였음.

### 3-1-2. 고품질 AI 음성 합성 및 Voice Cloning 기술 확보

- RVC 및 XTTS를 통합하여 짧은 음성 샘플만으로 원화자의 음색(Timbre)과 억양(Prosody)을 유사하게 재현하는 기반을 확보하였음.
- VALL-E X/XTTS/Gemini 모델을 하이브리드로 운용하여 다국어 음성 합성의 자연스러움을 개선하였음.

### 3-1-3. 고해상도 립싱크 기술 고도화

- MuseTalk 도입을 통해 기존 저해상도 한계(입 주변부 Blurring)를 개선하고, 256x256 이상의 고해상도 립싱크를 구현하였음.
- MuseTalk 설정의 `bbox_shift` 및 `fp16` 옵션, Wav2Lip 설정의 `resize_factor`, `nosmooth`, `max_duration_sec` 등의 파라미터를 통해 추론 안정성과 처리 효율을 튜닝하였음.

### 3-1-4. GPU 가속 및 추론 효율성 개선

- GPU 활용 옵션(`use_gpu`)을 중심으로 가능한 경우 GPU에서 추론하도록 구성하였음.
- MuseTalk 립싱크 단계에서 fp16 옵션을 제공하여 추론 효율을 확보하도록 하였음.
- TorchScript/ONNX Runtime/TensorRT 기반 변환 최적화는 기술 검토를 수행하였으나, 변환 리스크로 인해 보류하고 캐싱 중심의 런타임 최적화로 방향을 설정하였음.

### 3-1-5. 사용자 중심 통합 플랫폼 구축

- Streamlit 기반 대시보드를 통해 영상 업로드, 언어/화자 선택, 자막 수정, 결과물 미리보기/다운로드 기능을 제공하였음.
- WER/BLEU/PSNR 등 지표 산출 스크립트 및 결과(JSON) 기반으로 품질을 계측·관리하는 체계를 마련하였음.

## 3-2. 정량적 연구개발성과(주요 성능지표)

본 과제에서는 IRIS 계획서 상 정량목표를 기준으로 품질 지표를 관리하였으며, 내부 검증에서는 대표 샘플을 대상으로 WER/BLEU/PSNR 등을 우선 계측하였음. 본 보고서에서는 실제 산출물이 확인되는 내부 계측 결과를 우선 제시하고, 목표치/미측정 항목은 명확히 구분하여 기술하였음.

### 3-2-0. IRIS 계획서 기준 목표치(정의)

| 구분 | 성능 지표 | 단위 | 목표치 |
| --- | --- | --- | --- |
| 음성 생성 품질 | MOS | 점 | 4.3 이상 |
| 음성 인식 정확도 | WER | % | 6.5 이하 |
| 기계 번역 품질 | BLEU | 점 | 41.0 이상 |
| 영상 품질 | PSNR | dB | 35 이상 |
| 생성 품질 | FID | 점 | 9 이하 |

- 근거(내부): `test_reports_company/reports/2025_phase3_system_v1_test_report_draft.md` 5.2절(IRIS 연구개발계획서 정량목표 요약 표)

### 3-2-1. 대표 샘플 기반 내부 계측 결과(초기)

| 지표 | 대표 실측값(요약) | 근거(내부 파일) | 비고 |
| --- | --- | --- | --- |
| WER(normalized) | 한국어(민형배) Whisper medium: 17.143% | `test_reports_company/data/b_metrics/wer_bleu/wer_results.minhb.medium.normalized.json` | 단일 샘플(토큰 105) |
| WER(normalized) | 한국어(민형배) Whisper large-v3: 10.476% | `test_reports_company/data/b_metrics/wer_bleu/wer_results.minhb.largev3.normalized.json` | 단일 샘플(토큰 105) |
| WER(normalized) | 영어(슈취타 EP.12 일부 블록) medium: 0.0% | `test_reports_company/data/b_metrics/wer_bleu/wer_results.ep12_block14.medium.normalized.json` 등 | 짧은 구간(토큰 6~8) |
| BLEU | 한국어(민형배) medium: 44.23 | `test_reports_company/data/b_metrics/wer_bleu/bleu_results.minhb.medium.json` | 1~4-gram corpus BLEU(0~100) |

- 최종 목표: 4.3점 이상
- 내부 기록: VALL-E X 연동 테스트 과정에서 MOS 4.3 달성(기존 3.8 대비 개선) 기록이 확인되었음.
  - 근거: (내부) `notev2/Kangchulwonv2/2025-04/20250416_HiFiGAN평가.md` 내 MOS 4.3(기존 3.8 대비 개선) 기록 / (내부) `Test_Report_limone_dev.pdf`는 이미지 기반 자료로 본문 수치(OCR) 재현·검증이 어려워, 수치 직접 인용은 보류하고 참고 자료로만 활용하였음 / (외부) ITU-T P.800.2(Mean opinion score interpretation and reporting): https://www.itu.int/rec/T-REC-P.800.2-201607-I/en , ITU-T P.800.1(Mean Opinion Score(MOS) terminology): https://www.itu.int/rec/T-REC-P.800.1-200303-S/en
  - 근거 스크린샷(외부): `docs/demo/external/ext_mos_itu_p8002.png`, `docs/demo/external/ext_mos_itu_p8001.png`

- 근거 발췌(연구노트):

> (notev2/Kangchulwonv2/2025-04/20250416_HiFiGAN평가.md) "**결과**: 기존 Vocoder 대비 기계음이 현저히 감소. 고음역대(High Frequency)가 선명해짐. MOS(Mean Opinion Score) 4.3 달성 (기존 3.8)."

- 정량 평가 체계: MOS는 청취 기반 주관평가 성격이 강하므로, 시험성적서(초안)에서는 보조적으로 SNR 기반 MOS 서러게이트를 설계하여 병행 평가하도록 계획하였음.
  - 근거(초안): `test_reports_company/reports/2025_phase3_system_v1_test_report_draft.md` 3.2절

### 3-2-3. WER/CER(음성 인식/발음 정확도)

- 최종 목표: WER 6.5% 이하
- 대표 샘플 계측(초기): 한국어(민형배) 샘플에서 normalized WER가 Whisper medium 17.143%, Whisper large-v3 10.476%로 계측되었음.
  - 근거: `test_reports_company/data/b_metrics/wer_bleu/wer_results.minhb.medium.normalized.json`, `test_reports_company/data/b_metrics/wer_bleu/wer_results.minhb.largev3.normalized.json`
 - 산출 방식(스크립트): JSONL 페어(`ref`, `hyp`)를 입력으로 하여 공백 기반 토큰화 후 Levenshtein 거리로 WER(%)를 계산하고, 참조 토큰 수 기준 가중 평균을 산출하였음.
  - 근거: (내부) `test_reports_company/data/b_metrics/wer_bleu/wer_results.*.json`(공개 제외) / (외부) NIST SCTK sclite(WER 정의): https://github.com/usnistgov/SCTK/blob/master/doc/sclite.htm
  - 근거 스크린샷(외부): `docs/demo/external/ext_wer_nist_sclite.png`

#### 3-2-3-1. 입력 데이터 포맷(JSONL)

WER 계측은 샘플 단위로 정답(ref)과 STT 출력(hyp)을 1:1로 매핑한 JSONL 파일을 입력으로 사용하였음.

- **[입력 포맷]** `{"id": "...", "ref": "정답", "hyp": "STT 출력"}` 형태의 JSON 오브젝트를 1줄 1샘플로 구성하였음.
- **[필수 필드]** `ref`, `hyp`가 누락되면 스크립트에서 에러로 처리하도록 구현하였음.

### 3-2-4. BLEU(번역 품질)

- 최종 목표: BLEU 41.0점 이상
- 대표 샘플 계측(초기): 한국어(민형배) 샘플 기준 BLEU가 Whisper medium 44.23, Whisper large-v3 41.50으로 계측되어 목표치(41.0) 이상을 달성하였음.
  - 근거: `test_reports_company/data/b_metrics/wer_bleu/bleu_results.minhb.medium.json`, `test_reports_company/data/b_metrics/wer_bleu/bleu_results.minhb.largev3.json`
 - 산출 방식(스크립트): 공백 기반 토큰화 후 1~4-gram corpus BLEU를 0~100 스케일로 산출하도록 구현하였음.
  - 근거: (내부) `test_reports_company/data/b_metrics/wer_bleu/bleu_results.*.json`(공개 제외) / (외부) Papineni et al., 2002, "Bleu: a Method for Automatic Evaluation of Machine Translation": https://aclanthology.org/P02-1040/
  - 근거 스크린샷(외부): `docs/demo/external/ext_bleu_acl_p02_1040.png`

#### 3-2-4-1. 입력 데이터 포맷(JSONL)

BLEU 계측은 WER과 동일한 JSONL 페어(`id/ref/hyp`) 포맷을 사용하였음.

### 3-2-5. PSNR/FID(립싱크 영상 품질)

- 최종 목표: PSNR 35dB 이상 / FID 9점 이하
- 대표 샘플 계측(초기): Wav2Lip 통합 시뮬레이션 영상 5건에서 ref/gen 페어가 동일하여 PSNR이 Infinity로 계측되었음(MSE=0). 이는 파이프라인 통합이 프레임 손실 없이 동작함을 확인하는 스모크 테스트 성격임.
  - 근거: `test_reports_company/data/b_metrics/psnr_fid/video_quality_results.wav2lip_integration.json`
 - 산출 방식(스크립트): OpenCV로 두 영상의 프레임을 순차 로딩하여 MSE를 계산하고 PSNR(dB)을 산출하도록 구현하였음.
  - 근거: (내부) `test_reports_company/data/b_metrics/psnr_fid/video_quality_results.*.json`(공개 제외) / (외부) MathWorks PSNR 함수 문서(PSNR/MSE 정의 및 peak value 설명): https://www.mathworks.com/help/images/ref/psnr.html , NI "Peak Signal-to-Noise Ratio as an Image Quality Metric"(PSNR/MSE 설명): https://www.ni.com/en/shop/data-acquisition-and-control/add-ons-for-data-acquisition-and-control/what-is-vision-development-module/peak-signal-to-noise-ratio-as-an-image-quality-metric.html , OpenCV 공식 튜토리얼(PSNR 정의 및 MSE 기반 계산): https://docs.opencv.org/4.x/d5/dc4/tutorial_video_input_psnr_ssim.html
  - 근거 스크린샷(외부): `docs/demo/external/ext_psnr_mathworks.png`, `docs/demo/external/ext_psnr_ni.png`, `docs/demo/external/ext_psnr_opencv.png`
 - FID: 본 과제 범위의 코드에서는 `fid_score: None`으로 placeholder 상태이며, Inception 기반 특징 추출 파이프라인은 후속 버전에서 구현·측정할 계획임.
  - 근거: (외부) Heusel et al., 2017, "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium" (arXiv:1706.08500), https://arxiv.org/abs/1706.08500
  - 근거 스크린샷(외부): `docs/demo/external/ext_fid_arxiv_1706_08500.png`

#### 3-2-5-1. 입력 데이터 포맷(JSON)

PSNR/FID 계측 스크립트는 JSONL이 아니라, 비디오 페어 리스트(JSON 배열)를 입력으로 사용하도록 구현하였음.
- **[필수 필드]** 각 항목에 `ref`, `gen` 필드가 없으면 에러로 처리하도록 구현하였음.

#### 3-2-5-2. PSNR 계산 정의(MSE 기반)

- **[프레임 로딩]** OpenCV의 `cv2.VideoCapture`로 ref/gen 영상을 동시에 프레임 단위로 로딩하도록 구현하였음.
- **[해상도 정합]** 프레임 해상도가 다르면 gen을 ref 해상도에 맞춰 리사이즈하여 비교하도록 구현하였음.
- **[MSE]** 모든 프레임의 픽셀 단위 제곱 오차를 누적하여 전체 MSE를 계산하였음.
- **[PSNR(dB)]** `PSNR = 10 * log10((255^2) / MSE)`로 계산하였으며, `MSE == 0`인 경우 무한대(`Infinity`)로 처리하도록 구현하였음.

#### 3-2-5-3. 통합 시뮬레이션 결과의 의미 및 한계

- **[Infinity 해석]** 현재 보고서에 반영된 PSNR=Infinity는 ref/gen 영상이 동일하여 손실이 0인 경우에 발생하며, 립싱크 품질이 높다는 의미가 아니라 파일 페어링/통합 경로가 정상 동작했음을 확인하는 스모크 테스트 성격임.
- **[실제 립싱크 품질 평가 필요]** 민형배/슈취타 등 실제 더빙 결과의 PSNR/FID 계측은 ref(원본)와 gen(립싱크 결과)의 정의를 재정의한 뒤 재측정이 필요함.

#### 3-2-5-4. FID 측정 계획

- **[현 상태]** FID 자동 측정은 미구현 상태이며, 내부 결과 JSON에도 `fid_score: None` 형태로 placeholder가 기록되어 있음.
- **[후속 계획]** Inception 기반 특징 추출 및 분포 거리 계산 파이프라인을 구현하여, 동일 입력 조건에서 FID를 산출하고 목표치(9 이하) 달성 여부를 검증할 계획임.

# 4. 목표 미달 시 원인분석(해당 시 작성)

본 과제에서는 연구개발계획서에서 제시한 핵심 정량 목표(음성 유사도, 학습 시간, 립싱크 정합도, 처리 속도, 자막 정합률 등)에 대하여, 대체로 계획된 목표 수준에 도달하거나 이에 근접하는 성능을 달성하였으므로, 뚜렷한 의미의 목표 미달 항목은 없었음.

- 근거(정성): 내부 통합 테스트 및 연구노트 기반으로 주요 기능 목표를 달성하였으며, 일부 정량 목표는 대표 샘플 계측 결과가 목표에 근접함을 확인하였음. 다만 `Test_Report_limone_dev.pdf`는 이미지 기반 자료로 텍스트 수준 재현·검증이 어려워, 결론 문구의 직접 인용은 보류하고 참고 자료로만 활용하였음.

다만, 장시간(장문) 영상 처리 구간에서의 추가적인 지연 시간 단축, 중국 등 해외 네트워크 환경에서의 CDN 성능 편차 완화, 한국어 외 추가 언어·도메인(예: 특수 교육 콘텐츠, 콜센터 대화 등)에 대한 성능 고도화는 과제 기간 내에 부분적으로만 검증되었으며, 후속 연구개발을 통해 지속적인 개선이 필요한 과제로 남아 있음. 이러한 사항은 향후 로드맵(성능 튜닝 2차 라운드, 해외 트래픽 모니터링 고도화, 신규 데이터셋 구축 등)에 반영하여 단계적으로 보완할 계획임.

# 5. 연구개발성과 및 관련 분야에 대한 기여 정도

본 과제의 연구개발성과는 AI 음성 합성, 립싱크, 멀티모달 딥러닝, GPU 최적화 및 MLOps/서비스 운영 등 관련 기술 분야에 대하여 다음과 같은 기여를 하였음.

## 5-1. 고품질 AI 음성 합성 및 다국어 TTS 분야 기여

RVC를 활용한 화자 음색 보존 기법과 VALL‑E X 기반 다국어 TTS를 서비스 수준 파이프라인에 통합함으로써, 원 화자의 음색을 유지하면서도 자연스러운 다국어 음성을 생성할 수 있는 하이브리드 아키텍처를 구현하였음. 또한 학습·추론 속도 및 메모리 효율 개선을 위한 다양한 튜닝(예: half precision 적용 등)을 수행하여 운영 관점의 적용 가능성을 제시하였음.

## 5-2. AI 기반 립싱크 및 멀티모달 영상 합성 분야 기여

Wav2Lip 기반 립싱크 모델을 실서비스 환경에 맞게 개선하고, MuseTalk 도입을 통해 고해상도 립싱크 품질을 확보하였음. 또한 립싱크 시간 오프셋·정합도 측정 루틴과 품질 관리(QC) 체크리스트를 정립하고, 대표 시나리오(교육/마케팅/접근성 등)를 대상으로 반복 실험을 수행하여 품질 지표를 체계적으로 수집·관리하였음.

## 5-3. AI 더빙 워크플로우 및 MLOps/서비스 운영 분야 기여

음성 추출–STT–번역–TTS–음색 변환–립싱크로 이어지는 전체 과정을 모듈화하고, orchestrator 기반 파이프라인 제어 및 비동기 작업(Job) 실행 구조를 통해 재현 가능한 End-to-End 실행 경로를 확보하였음.
  - 근거: `orchestrator/pipeline_runner.py`, `orchestrator/config.yaml`, `backend/main.py`, `backend/job_manager.py`, `backend/routers/*`, `frontend_unified/Home.py`, `frontend_unified/pages/*`, `frontend_unified/steps/*`

또한 모델 변환 기반 가속(ONNX Runtime/TensorRT)은 기술 검토를 수행하였으나 변환 리스크가 확인되어 보류하고, 캐싱 중심의 런타임 최적화로 방향을 설정하는 의사결정을 문서화함으로써 운영 관점의 현실적인 최적화 전략을 제시하였음.
  - 근거: (내부) `notev2/Kangchulwonv2/2025-04/20250409_모델서빙최적화.md` / (외부) ONNX Runtime: https://onnxruntime.ai/docs/ , NVIDIA TensorRT: https://developer.nvidia.com/tensorrt

추가로 시험랩(`test_reports_company`) 기반의 지표 계측/시험 절차 문서를 정비하여, 동일 데이터·동일 커밋에서 재현 가능한 평가 흐름을 확보하였음.
  - 시험 절차서(SOP) 및 측정 실행 플로우 문서화: 환경 준비→샘플 준비→성능 시험→품질 시험→운영/IRIS 흐름 시험→결과 정리의 상위 절차와, 벤치마크/품질 측정 실행 명령을 문서로 정리하였음.
  - 근거: `test_reports_company/docs/process/testing_sop_v1.md`, `test_reports_company/docs/process/measurement_commands_v1.md`
- B지표 계측 계획 수립: WER/BLEU/PSNR/FID/MOS의 정의, 입력 포맷, 스크립트 경로, 결과 저장 구조(JSON)를 명시하여 시험성적서와 연결 가능한 형태로 관리하였음.
  - 근거: `test_reports_company/docs/process/b_metrics_plan_v1.md`, `test_reports_company/data/b_metrics/**`
- MOS 보조 지표(서러게이트) 설계: 청취 기반 MOS의 운용 한계를 보완하기 위해, SNR(dB)→MOS(1~5) 선형 보간 기반의 서러게이트 집계 스크립트를 구성하여 보조 지표로 활용 가능하도록 하였음.
  - 근거: `test_reports_company/scripts/aggregate_mos_from_snr.py`

# 6. 연구개발성과의 관리 및 활용 계획

본 과제에서 개발한 AI 기반 다국어 더빙 시스템과 관련 모델·파이프라인·운영 도구는 향후 안정적인 서비스 운영과 추가 고도화를 위하여 다음과 같이 관리·활용할 계획임.

## 6-1. 시스템 및 모델 자산의 체계적 관리

- 소스코드/설정은 형상관리(Git) 기반으로 브랜치 전략 및 코드 리뷰 절차에 따라 관리할 계획임.
- 핵심 모델과 학습 체크포인트는 버전별로 보관하여 재현 가능성을 확보하고, 성능 저하나 운영 이슈 발생 시 신속한 롤백/재현이 가능하도록 할 계획임.
- 운영 문서(Runbook/SOP)는 정기적으로 점검·갱신하여 장애 대응 및 운영 안정성을 확보할 계획임.

## 6-2. 서비스/사업 적용 및 확산

- 내부 프로젝트(교육/홍보/접근성 영상 제작 등)에 우선 적용하여 작업 시간 단축 및 품질 향상 효과를 지속 검증할 계획임.
- 안정성이 검증된 모듈(음성 합성 API, 립싱크 모듈, QC 자동 평가 등)은 외부 파트너(교육기관, 콘텐츠 제작사, 공공기관 등)에 API 또는 콘솔 형태로 제공하는 방안을 검토할 계획임.

## 6-3. 후속 R&D 및 지식 확산

- 성능 지표 체계(지연 시간, 립싱크 정합도, 음색 유사도, 자막 정합률 등)와 시험·평가 시나리오, 품질 관리 체크리스트 등을 내부 가이드라인으로 정리하여 신규 인력 교육 및 후속 과제 기획에 활용할 계획임.
- 필요 시 논문/기술 보고서/세미나 등을 통해 핵심 알고리즘 및 운영 경험을 공유하여 관련 분야 발전에 기여할 계획임.

# 별첨자료(참고 문헌 등)

## 참고문헌

- Wang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., ... & Wei, F. (2023). Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers. arXiv:2301.02111.
- Casanova, E., Davis, K., Gölge, E., Göknar, G., Gulea, I., Hart, L., ... & Weber, J. (2024). XTTS: a Massively Multilingual Zero-Shot Text-to-Speech Model. arXiv:2406.04904.
- Google Gemini App Expands Audio Support: A New Era for Multi-Modal AI Workflows. https://applyingai.com/2025/09/google-gemini-app-expands-audio-support-a-new-era-for-multi-modal-ai-workflows/
- Han, B., Zhou, L., Liu, S., Chen, S., Meng, L., Qian, Y., ... & Wei, F. (2024). VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment. arXiv:2406.07855.
- Patil, A., Tao, S., & Jadon, A. (2024). English Please: Evaluating Machine Translation with Large Language Models for Multilingual Bug Reports. arXiv:2408.02162.
- Keles, B., Gunay, M., & Caglar, S. I. (2024). LLMs-in-the-loop Part-1: Expert Small AI Models for Bio-Medical Text Translation. arXiv:2405.04368.
- Mallikarjuna, G. D., Basha, M. J., Kumar, A. S., & Abhishek, S. (2024). Wav2Lip-HQ: High-Resolution Audio-Driven Lip Synchronization for Realistic Virtual Avatars. IJARSCT, 4(1).
- Zhang, Y., Liu, M., Chen, Z., Wu, B., Zeng, Y., Zhan, C., ... & Zhou, W. (2024). MuseTalk: Real-Time High Quality Lip Synchronization with Latent Space Inpainting. arXiv:2410.10122.
